{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Object Detection Tutorial using Labelme**\n",
        "1. Run inferences on images, with existing detectron2 models\n",
        "2. Train detectron 2 with new dataset\n",
        "3. Evaluate model with adapted presence-based metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "2pxkUSLse_ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "# Install detectron 2 and import other packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "outputs": [],
      "source": [
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "!pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "!pip install piexif\n",
        "!pip install torch\n",
        "!pip install cv2-plt-imshow\n",
        "!pip install pandas\n",
        "!pip install seaborn\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6psJfQGeRA7J"
      },
      "outputs": [],
      "source": [
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, random\n",
        "import matplotlib.pyplot as plt\n",
        "from cv2_plt_imshow import cv2_plt_imshow, plt_format\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.config import CfgNode as CN\n",
        "\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, DatasetMapper, build_detection_train_loader\n",
        "from detectron2.data.samplers import RepeatFactorTrainingSampler\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
        "from detectron2.engine.hooks import HookBase\n",
        "\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, inference_context\n",
        "\n",
        "from detectron2.structures import BoxMode, Boxes, pairwise_iou\n",
        "\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "import detectron2.utils.comm as comm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHj9sl8k1Yzm"
      },
      "source": [
        "# Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "## Link to google drive, download and unzip data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q65ZebgzKOz"
      },
      "outputs": [],
      "source": [
        "#Link to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgfWx3uBzLMF"
      },
      "outputs": [],
      "source": [
        "#Insert the file ID  where you uploaded your dataset into google drive\n",
        "!gdown --id 1sxd42J3SwFRVeBK45_oDdJs6F_AG-w1N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTi_6UXdzTst"
      },
      "outputs": [],
      "source": [
        "#Unzip file\n",
        "!unzip -o 影像集_noaphidandthrip_updated_0221.zip > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuH2G5_oy4V-"
      },
      "source": [
        "## Set environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYfSGnnUzghi"
      },
      "outputs": [],
      "source": [
        "#SET ENVIRONMENT VARIABLES\n",
        "BASE_DIR = '/content'\n",
        "DATASET_DIR = f'{BASE_DIR}/影像集_noaphidandthrip_updated_final'\n",
        "OUTPUT_DIR = f'{BASE_DIR}/output'\n",
        "DATE_TIME = '0330_2015'\n",
        "\n",
        "#SET CATEGORIES (Copy and pasted from the last line of JSON file )\n",
        "categories = [{\"id\": 0, \"name\": \"fbeetle_b\", \"supercategory\": \"fbeetle_b\"}, {\"id\": 1, \"name\": \"mealy_b\", \"supercategory\": \"mealy_b\"}, {\"id\": 2, \"name\": \"sooty_l\", \"supercategory\": \"sooty_l\"}, {\"id\": 3, \"name\": \"spmite_l\", \"supercategory\": \"spmite_l\"}, {\"id\": 4, \"name\": \"lfworm_b\", \"supercategory\": \"lfworm_b\"}, {\"id\": 5, \"name\": \"spmite_b\", \"supercategory\": \"spmite_b\"}, {\"id\": 6, \"name\": \"wfly_b\", \"supercategory\": \"wfly_b\"}]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut_ojD_wzweH"
      },
      "source": [
        "## Convert labelme to COCOjson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0SmyJnDCDZ0"
      },
      "outputs": [],
      "source": [
        "#SET AND CREATE JSON FILES\n",
        "json_files = {\n",
        "    'train': os.path.join(DATASET_DIR, 'train.json'),\n",
        "    'val': os.path.join(DATASET_DIR, 'val.json'),\n",
        "    'test': os.path.join(DATASET_DIR, 'test.json')\n",
        "}\n",
        "\n",
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "def get_detectron_dicts(img_dir, json_file):\n",
        "    \"\"\"\n",
        "\n",
        "    Register COCO Json dataset format to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n",
        "    A function is used to parse and prepare it into detectron2's standard format. Load every image an their respective information in specific JSON file, and load into a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - img_dir (float): Image directory\n",
        "    - json_file (float): JSON file path (train.json, val.json, test.json)\n",
        "\n",
        "    Returns:\n",
        "    dataset_dicts: Return a detectron2-formatted dataset\n",
        "\n",
        "    \"\"\"\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for img_ann in imgs_anns['images']:\n",
        "        record = {}\n",
        "        filename = os.path.join(img_dir, img_ann['file_name'])\n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = img_ann['id']\n",
        "        record[\"height\"] = img_ann['height']\n",
        "        record[\"width\"] = img_ann['width']\n",
        "        annos = [anno for anno in imgs_anns['annotations'] if anno['image_id'] == img_ann['id']]\n",
        "\n",
        "        objs = []\n",
        "        for anno in annos:\n",
        "            obj = {\n",
        "                \"bbox\": anno['bbox'],\n",
        "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "                \"category_id\": anno['category_id'],\n",
        "                \"iscrowd\": 0\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "def unregister_dataset(name):\n",
        "    \"\"\"\n",
        "    Unregister a dataset from both DatasetCatalog and MetadataCatalog\n",
        "    \"\"\"\n",
        "    if name in DatasetCatalog.list():\n",
        "        DatasetCatalog.remove(name)\n",
        "    if name in MetadataCatalog.list():\n",
        "        MetadataCatalog.remove(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For every dataset, register into a detectron2 format\n",
        "def create_dataset_loader_func(img_dir, json_file):\n",
        "    return lambda: get_detectron_dicts(img_dir, json_file)\n",
        "\n",
        "for d in [\"train\", \"val\", \"test\"]:\n",
        "    dataset_name = \"my_dataset_\" + d\n",
        "    json_file_path = json_files[d]\n",
        "    unregister_dataset(dataset_name)  # Unregister if already registered\n",
        "    DatasetCatalog.register(dataset_name, create_dataset_loader_func(DATASET_DIR, json_file_path))\n",
        "    MetadataCatalog.get(dataset_name).set(thing_classes=[cat[\"name\"] for cat in categories])"
      ],
      "metadata": {
        "id": "Kmh8sEN_o-1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8d-MaoKhxSv"
      },
      "outputs": [],
      "source": [
        "#Register dataset and metadata into catalog\n",
        "my_dataset_train = DatasetCatalog.get(\"my_dataset_train\")\n",
        "my_dataset_test = DatasetCatalog.get(\"my_dataset_test\")\n",
        "my_dataset_val = DatasetCatalog.get(\"my_dataset_val\")\n",
        "my_dataset_metadata_train = MetadataCatalog.get(\"my_dataset_train\")\n",
        "my_dataset_metadata_test = MetadataCatalog.get(\"my_dataset_test\")\n",
        "my_dataset_metadata_val = MetadataCatalog.get(\"my_dataset_val\")\n",
        "\n",
        "print(my_dataset_metadata_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvF0V98zR3hP"
      },
      "source": [
        "## Show Training Data and Class distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yooKtKzWNDy2"
      },
      "outputs": [],
      "source": [
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "my_dataset_train = DatasetCatalog.get(\"my_dataset_train\")\n",
        "my_dataset_metadata_train = MetadataCatalog.get(\"my_dataset_train\")\n",
        "\n",
        "for d in random.sample(my_dataset_train, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])  # Read image with OpenCV\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    visualizer = Visualizer(img_rgb, metadata=my_dataset_metadata_train, scale=1)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    plt.imshow(out.get_image())\n",
        "    plt.title('Filename: ' + d[\"file_name\"])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "# Model Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQJQNtrlYQ0F"
      },
      "source": [
        "## Create Repeat Factor Training Sampler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created a repeat factor training sampler because of class imbalance between the categories. The repeat factor training sampler calculates the number of instances across categories, and samples images of less represented categoreis."
      ],
      "metadata": {
        "id": "-fdhK1FDugiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYI0B5haVtp_"
      },
      "outputs": [],
      "source": [
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "\n",
        "# Referenced issue for RepeatFactorTrainingSampler usage explanation:\n",
        "# amroybd. (2021, November 25). Explanation of the RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(repeat_thresh) [Issue]. GitHub repository. https://github.com/facebookresearch/detectron2/issues/4095\n",
        "\n",
        "\n",
        "def get_repeat_factors(data):\n",
        "    \"\"\"\n",
        "\n",
        "    Create repeat factors based on the frequency of category and instance counts.\n",
        "    First calculate the category with highest instance counts, then use it to\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the frequency of each category using a dictionary\n",
        "    category_counts = defaultdict(int)\n",
        "    for record in data:\n",
        "        for anno in record[\"annotations\"]:\n",
        "            category_counts[anno[\"category_id\"]] += 1\n",
        "\n",
        "    # Get the maximum/highest count\n",
        "    max_count = max(category_counts.values())\n",
        "\n",
        "    # Calculate repeat factors for each image\n",
        "    # Now considering all annotations in each image\n",
        "    repeat_factors = []\n",
        "    for record in data:\n",
        "        unique_categories = set(anno[\"category_id\"] for anno in record[\"annotations\"])\n",
        "        # Fr images with multiple categories, Average the factors for all categories present in the image\n",
        "        factors = [max_count / category_counts[cat_id] for cat_id in unique_categories]\n",
        "        avg_factor = sum(factors) / len(factors)\n",
        "        repeat_factors.append(avg_factor)\n",
        "    return repeat_factors\n",
        "\n",
        "\n",
        "def build_custom_train_loader(cfg, dataset_name):\n",
        "    \"\"\"\n",
        "\n",
        "    Build custom train loader using repeat factor training sampler\n",
        "\n",
        "    \"\"\"\n",
        "    train_data = DatasetCatalog.get(dataset_name)\n",
        "    repeat_factors = get_repeat_factors(train_data)\n",
        "    repeat_factors_tensor = torch.tensor(repeat_factors)\n",
        "    sampler = RepeatFactorTrainingSampler(repeat_factors=repeat_factors_tensor, shuffle=True)\n",
        "    total_batch_size = 4\n",
        "\n",
        "    #\n",
        "    sampled = defaultdict(int)\n",
        "    num_categories = max([anno[\"category_id\"] for data in train_data for anno in data[\"annotations\"]]) + 1  # Assuming category_ids start from 0\n",
        "    num_iterations = 1000  # or whatever number you want\n",
        "\n",
        "    for i, sample in enumerate(sampler):\n",
        "        for anno in train_data[sample][\"annotations\"]:\n",
        "            category_id = anno[\"category_id\"]\n",
        "            sampled[category_id] += 1\n",
        "\n",
        "        if i == num_iterations:\n",
        "            break\n",
        "\n",
        "    # Create a histogram\n",
        "    histogram = torch.zeros(num_categories)\n",
        "    for cat_id, count in sampled.items():\n",
        "        histogram[cat_id] = count\n",
        "\n",
        "    # Normalize the histogram to get a distribution\n",
        "    distribution = histogram / num_iterations\n",
        "    print(\"Sampled distribution:\", distribution)\n",
        "\n",
        "    # If you want a more detailed printout:\n",
        "    for cat_id, freq in enumerate(distribution):\n",
        "        print(f\"Category {cat_id}: {freq:.4f}\")\n",
        "\n",
        "    return build_detection_train_loader(\n",
        "        train_data,\n",
        "        sampler=sampler,\n",
        "        mapper=DatasetMapper(cfg, is_train=True),\n",
        "        total_batch_size=total_batch_size\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P1CnokSfO3l"
      },
      "source": [
        "## Create LossEvalHook for Validation Loss Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-r599Bi2OhB"
      },
      "outputs": [],
      "source": [
        "# Adapted from eidos.ai. (2020, March 22). Training on Detectron2 with a Validation set, and plot loss on it to avoid overfitting.\n",
        "#     Medium. https://eidos-ai.medium.com/training-on-detectron2-with-a-validation-set-and-plot-loss-on-it-to-avoid-overfitting-6449418fbf4e\n",
        "\n",
        "class LossEvalHook(HookBase):\n",
        "    \"\"\"\n",
        "\n",
        "    Create a Loss Evaluation Hook to evaluate validation loss every few evaluation period in Detectron 2\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialise model and evaluation period\n",
        "    def __init__(self, eval_period, model, data_loader):\n",
        "        self._model = model\n",
        "        self._period = eval_period\n",
        "        self._data_loader = data_loader\n",
        "\n",
        "    #Run Loss on Validation\n",
        "    def _do_loss_eval(self):\n",
        "        # Copying inference_on_dataset from evaluator.py\n",
        "        total = len(self._data_loader)\n",
        "        num_warmup = min(5, total - 1)\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "        total_compute_time = 0\n",
        "        losses = []\n",
        "        for idx, inputs in enumerate(self._data_loader):\n",
        "            if idx == num_warmup:\n",
        "                start_time = time.perf_counter()\n",
        "                total_compute_time = 0\n",
        "            start_compute_time = time.perf_counter()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            total_compute_time += time.perf_counter() - start_compute_time\n",
        "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
        "            seconds_per_img = total_compute_time / iters_after_start\n",
        "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
        "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
        "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
        "                log_every_n_seconds(\n",
        "                    logging.INFO,\n",
        "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
        "                        idx + 1, total, seconds_per_img, str(eta)\n",
        "                    ),\n",
        "                    n=5,\n",
        "                )\n",
        "            loss_batch = self._get_loss(inputs)\n",
        "            losses.append(loss_batch)\n",
        "        mean_loss = np.mean(losses)\n",
        "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
        "        comm.synchronize()\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def _get_loss(self, data):\n",
        "        # How loss is calculated on train_loop\n",
        "        metrics_dict = self._model(data)\n",
        "        metrics_dict = {\n",
        "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
        "            for k, v in metrics_dict.items()\n",
        "        }\n",
        "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
        "        return total_losses_reduced\n",
        "\n",
        "    def after_step(self):\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        is_final = next_iter == self.trainer.max_iter\n",
        "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
        "            self._do_loss_eval()\n",
        "        self.trainer.storage.put_scalars(timetest=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJCE4a-83Uy-"
      },
      "source": [
        "## Define Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GxaqAi5fKzb"
      },
      "outputs": [],
      "source": [
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "\n",
        "#Initialise Model\n",
        "cfg = get_cfg()\n",
        "\n",
        "#Reference pre-trained models from model zoo (Wu, 2019)\n",
        "cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml'))\n",
        "\n",
        "#Import dataset into configurations\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
        "\n",
        "#Model ROI\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 190 #defines number of region proposals per image\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7  # 7 different disease types\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8 #only detections with confidence score above 0.7 will be considered\n",
        "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5 # dictates how much overlap is allowed between the proposed regions\n",
        "\n",
        "#Model Weights: Initialised from pre-trained checkpoint (transfer learning)\n",
        "cfg.MODEL.WEIGHTS =  model_zoo.get_checkpoint_url('COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml')\n",
        "\n",
        "# Solver Configurations\n",
        "cfg.SOLVER.OPTIMIZER = \"Adam\"\n",
        "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
        "cfg.SOLVER.IMS_PER_BATCH = 120\n",
        "cfg.SOLVER.BASE_LR = 0.0005\n",
        "cfg.SOLVER.MOMENTUM = 0.9 #safe and effective choice\n",
        "cfg.SOLVER.WEIGHT_DECAY = 0.000015 #from 0.00001 prevent overfitting --> if you see it is overfitting, increase it\n",
        "cfg.SOLVER.GAMMA = 0.1 #safe and effective choice too\n",
        "cfg.SOLVER.WARMUP_ITERS = 100\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.SOLVER.STEPS = (600, 800)\n",
        "cfg.SOLVER.WARMUP_METHOD = \"linear\"\n",
        "cfg.SOLVER.WARMUP_FACTOR = 0.001\n",
        "\n",
        "#Evaluate\n",
        "cfg.TEST.EVAL_PERIOD = 100\n",
        "\n",
        "#Number of DataLoaders --> Depends on GPU\n",
        "cfg.DATALOADER.NUM_WORKERS = 8\n",
        "\n",
        "#A Custom Trainer is used to perform Repeat Trainer Sampling to ensure balance between the classes\n",
        "class CustomTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "\n",
        "    #Create hook that does validation loss calculation\n",
        "    def build_hooks(self):\n",
        "        hooks = super().build_hooks()\n",
        "        hooks.insert(-1,LossEvalHook(\n",
        "            cfg.TEST.EVAL_PERIOD,\n",
        "            self.model,\n",
        "            build_detection_test_loader(\n",
        "                self.cfg,\n",
        "                self.cfg.DATASETS.TEST[0],\n",
        "                DatasetMapper(self.cfg,True)\n",
        "            )\n",
        "        ))\n",
        "        return hooks\n",
        "\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_custom_train_loader(cfg, \"my_dataset_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5xJk4gsRA7O"
      },
      "outputs": [],
      "source": [
        "# Create Output Directory\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY5999If3jpt"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq7BMzp9hsGb"
      },
      "outputs": [],
      "source": [
        "#Train model\n",
        "trainer = CustomTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxdQUI1pRA7O"
      },
      "outputs": [],
      "source": [
        "MODEL_SAVE_PATH = f'{BASE_DIR}/my_trained_model_{DATE_TIME}_faster_rcnn_X_101_32x8d_FPN_3x.yaml'\n",
        "\n",
        "# Save the model after training to Google Drive\n",
        "torch.save(trainer.model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mExywn5g6Syj"
      },
      "outputs": [],
      "source": [
        "#Save to google drive\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# Current date and time for unique file names\n",
        "DATE_TIME = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "MODEL_SAVE_PATH = f'/content/drive/My Drive/my_trained_model_faster_rcnn_X_101_32x8d_FPN_3x.yaml-{DATE_TIME}.pth'\n",
        "\n",
        "# Save the model after training to the path\n",
        "torch.save(trainer.model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APBuLB5T3pqe"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "outputs": [],
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbwVc8lT2atC"
      },
      "outputs": [],
      "source": [
        "# Adapted from eidos.ai. (2020, March 22). Training on Detectron2 with a Validation set, and plot loss on it to avoid overfitting.\n",
        "# Medium. https://eidos-ai.medium.com/training-on-detectron2-with-a-validation-set-and-plot-loss-on-it-to-avoid-overfitting-6449418fbf4e\n",
        "\n",
        "experiment_folder = f'{BASE_DIR}/output'\n",
        "\n",
        "def load_json_arr(json_path):\n",
        "    lines = []\n",
        "    with open(json_path, 'r') as f:\n",
        "        for line in f:\n",
        "            lines.append(json.loads(line))\n",
        "    lines.sort(key=lambda x: x.get('iteration', 0))\n",
        "    return lines\n",
        "\n",
        "experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
        "\n",
        "# Extracting total_loss and validation_loss\n",
        "total_loss = [(x['iteration'], x['total_loss']) for x in experiment_metrics if 'total_loss' in x]\n",
        "validation_loss = [(x['iteration'], x['validation_loss']) for x in experiment_metrics if 'validation_loss' in x]\n",
        "\n",
        "# Unzipping for plotting\n",
        "iterations_total, losses_total = zip(*total_loss) if total_loss else ([], [])\n",
        "iterations_validation, losses_validation = zip(*validation_loss) if validation_loss else ([], [])\n",
        "\n",
        "plt.plot(iterations_total, losses_total, label='Total Loss')\n",
        "plt.plot(iterations_validation, losses_validation, label='Validation Loss')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(\"Total Loss vs. Validation Loss Over Iterations\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF"
      },
      "source": [
        "# Inference & evaluation using the trained model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import DatasetMapper, build_detection_train_loader"
      ],
      "metadata": {
        "id": "goWnsCgp22xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya5nEuMELeq8"
      },
      "outputs": [],
      "source": [
        "#Reference model weights from final model\n",
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "evaluator = COCOEvaluator(\"my_dataset_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference model weights from saved model\n",
        "# Adapted from Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., & Girshick, R. (2019). Detectron2 Tutorial. [Software]. Retrieved from https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
        "cfg.MODEL.WEIGHTS = f'/content/drive/My Drive/my_trained_model_faster_rcnn_R_101_FPN_3x.yaml-20240224_122604.pth'  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "evaluator = COCOEvaluator(\"my_dataset_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
      ],
      "metadata": {
        "id": "dKlFQwLRVCp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Vrdmai5beF"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "## Precision Recall Curve\n",
        "- Confidence threshold\n",
        "- IoU threshold\n",
        "- NMS threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27NelSgQBDJm"
      },
      "outputs": [],
      "source": [
        "def calculate_pr_curve(confusion_matrix):\n",
        "    \"\"\"\n",
        "\n",
        "    Calculate micro-averaged precision and recall for classification problems\n",
        "\n",
        "    \"\"\"\n",
        "    # True Positives are the diagonal elements\n",
        "    TP = np.diag(confusion_matrix)\n",
        "\n",
        "    # Exclude background row in the mean time for PR curve\n",
        "    FP = np.sum(confusion_matrix[:-1, :], axis=1) - TP[:-1]\n",
        "    FN = np.sum(confusion_matrix[:, :-1], axis=0) - TP[:-1]\n",
        "\n",
        "    # Sum up TP, FP, and FN over all classes\n",
        "    total_TP = np.sum(TP)\n",
        "    total_FP = np.sum(FP)\n",
        "    total_FN = np.sum(FN)\n",
        "\n",
        "    # Micro-averaged Precision and Recall\n",
        "    micro_precision = total_TP / (total_TP + total_FP)\n",
        "    micro_recall = total_TP / (total_TP + total_FN)\n",
        "\n",
        "    return micro_precision, micro_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtUwEdoP4pSV"
      },
      "outputs": [],
      "source": [
        "#Adapted from stackoverflow response (Griffo, 2012)\n",
        "def coco_bbox_to_coordinates(bbox):\n",
        "    out = bbox.copy().astype(float)\n",
        "    out[:, 2] = bbox[:, 0] + bbox[:, 2]\n",
        "    out[:, 3] = bbox[:, 1] + bbox[:, 3]\n",
        "    return out\n",
        "\n",
        "def conf_matrix_calc(labels, detections, n_classes, filename, conf_thresh, iou_thresh):\n",
        "    confusion_matrix = np.zeros([n_classes + 1, n_classes + 1])\n",
        "    l_classes = np.array(labels)[:, 0].astype(int)\n",
        "    l_bboxs = coco_bbox_to_coordinates((np.array(labels)[:, 1:]))\n",
        "    d_confs = np.array(detections)[:, 4]\n",
        "    d_bboxs = (np.array(detections)[:, :4])\n",
        "    d_classes = np.array(detections)[:, -1].astype(int)\n",
        "    detections = detections[np.where(d_confs > conf_thresh)]\n",
        "    labels_detected = np.zeros(len(labels))\n",
        "    detections_matched = np.zeros(len(detections))\n",
        "    for l_idx, (l_class, l_bbox) in enumerate(zip(l_classes, l_bboxs)):\n",
        "        for d_idx, (d_bbox, d_class) in enumerate(zip(d_bboxs, d_classes)):\n",
        "            # if l_class != d_class:\n",
        "            #   print('didnt match', filename)\n",
        "            iou = pairwise_iou(Boxes(torch.from_numpy(np.array([l_bbox]))), Boxes(torch.from_numpy(np.array([d_bbox]))))\n",
        "            if iou >= iou_thresh:\n",
        "                confusion_matrix[l_class, d_class] += 1\n",
        "                labels_detected[l_idx] = 1\n",
        "                detections_matched[d_idx] = 1\n",
        "    for i in np.where(labels_detected == 0)[0]:\n",
        "        confusion_matrix[l_classes[i], -1] += 1\n",
        "    for i in np.where(detections_matched == 0)[0]:\n",
        "        confusion_matrix[-1, d_classes[i]] += 1\n",
        "    return confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-hBB-DdSH0m"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates pr curve across different thresholds\n",
        "\"\"\"\n",
        "\n",
        "setup_logger()\n",
        "\n",
        "conf_test = cfg\n",
        "\n",
        "# Placeholder for the PR curve data\n",
        "precision_recall_data = {}\n",
        "f1_scores = {}\n",
        "n_classes = 7\n",
        "confusion_matrix = np.zeros([n_classes + 1, n_classes + 1])\n",
        "\n",
        "# Thresholds to evaluate\n",
        "start, end, num_thresholds = 0.1, 0.9, 9\n",
        "step = (end - start) / (num_thresholds - 1)\n",
        "\n",
        "for i in range(num_thresholds):\n",
        "    thresh = start + i * step\n",
        "    conf_test.MODEL.ROI_HEADS.SCORE_THRESH_TEST = thresh  # Set the current threshold\n",
        "    predictor = DefaultPredictor(conf_test)\n",
        "\n",
        "    # Reset the confusion matrix for each threshold\n",
        "    confusion_matrix = np.zeros([n_classes + 1, n_classes + 1])\n",
        "\n",
        "    # Process each image in the test dataset\n",
        "    my_dataset_val = DatasetCatalog.get(\"my_dataset_val\")\n",
        "    for d in my_dataset_val:\n",
        "        img = cv2.imread(d[\"file_name\"])\n",
        "        outputs = predictor(img)\n",
        "        labels = list()\n",
        "        detections = list()\n",
        "        for ann in d[\"annotations\"]:\n",
        "            labels.append([ann[\"category_id\"]] + ann[\"bbox\"])\n",
        "\n",
        "        # Process model outputs to create detections\n",
        "        for coord, conf, cls in zip(\n",
        "            outputs[\"instances\"].get(\"pred_boxes\").tensor.cpu().numpy(),\n",
        "            outputs[\"instances\"].get(\"scores\").cpu().numpy(),\n",
        "            outputs[\"instances\"].get(\"pred_classes\").cpu().numpy()\n",
        "        ):\n",
        "            detections.append(list(coord) + [conf] + [cls])\n",
        "        if not labels:\n",
        "            labels.append([d['annotations'][0][\"category_id\"]] + d['annotations'][0][\"bbox\"])\n",
        "        # If detections are empty, add a single entry to \"null\" category\n",
        "        if not detections:\n",
        "            detections.append([0, 0, 0, 0, 0, 8])\n",
        "        confusion_matrix += conf_matrix_calc(np.array(labels), np.array(detections), n_classes, d[\"file_name\"], conf_thresh=thresh, iou_thresh=0.5)\n",
        "# Calculate precision and recall from the confusion matrix\n",
        "    # print(confusion_matrix)\n",
        "    precision, recall = calculate_pr_curve(confusion_matrix)\n",
        "    # print(precision, recall)\n",
        "    # Calculate F1 score for each threshold\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0 #for p, r in zip(precision, recall)]\n",
        "    max_f1 = f1_score #max(f1_score) if f1_score else 0  # Find the max F1 score\n",
        "\n",
        "    # Store the precision, recall, and F1 score\n",
        "    precision_recall_data[thresh] = (precision, recall, max_f1)\n",
        "    f1_scores[thresh] = max_f1\n",
        "\n",
        "# Now plot the PR curve for each threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for thresh, (precision, recall, _) in precision_recall_data.items():\n",
        "  plt.plot(recall, precision, marker='.', label=f'Threshold: {thresh:.2f}')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for different thresholds')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(f1_scores.items())\n",
        "# Print the F1 score for each threshold\n",
        "for thresh, f1_score in f1_scores.items():\n",
        "    print(thresh, f1_score)\n",
        "    print(f\"Threshold: {thresh:.2f}, Max F1 Score: {f1_score:.4f}\")"
      ],
      "metadata": {
        "id": "zwzqnt7xUPat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualuse testing data"
      ],
      "metadata": {
        "id": "idC2vhwoSn3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5LhISJqWXgM"
      },
      "outputs": [],
      "source": [
        "my_dataset_test = DatasetCatalog.get(\"my_dataset_test\")\n",
        "my_dataset_test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "for d in random.sample(my_dataset_val, 5):\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=my_dataset_test_metadata,\n",
        "                   scale=1)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    plt.imshow(out.get_image())\n",
        "    plt.title('Filename: ' + d[\"file_name\"])\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp0Mu-l-5uS_"
      },
      "source": [
        "## Create Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bowvyZYyT12K"
      },
      "outputs": [],
      "source": [
        "#Adapted from stackoverflow response (Griffo, 2012)\n",
        "# Set the current confidence threshold\n",
        "CONF_THRESH = 0.8\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Reset the confusion matrix for each threshold\n",
        "n_classes = 7\n",
        "confusion_matrix = np.zeros([n_classes + 1, n_classes + 1])\n",
        "\n",
        "# Process each image in the test dataset\n",
        "my_dataset_test = DatasetCatalog.get(\"my_dataset_test\")\n",
        "for d in my_dataset_test:\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(img)\n",
        "    labels = list()\n",
        "    detections = list()\n",
        "    for ann in d[\"annotations\"]:\n",
        "        labels.append([ann[\"category_id\"]] + ann[\"bbox\"])\n",
        "\n",
        "    # Process model outputs to create detections\n",
        "    for coord, conf, cls in zip(\n",
        "        outputs[\"instances\"].get(\"pred_boxes\").tensor.cpu().numpy(),\n",
        "        outputs[\"instances\"].get(\"scores\").cpu().numpy(),\n",
        "        outputs[\"instances\"].get(\"pred_classes\").cpu().numpy()\n",
        "    ):\n",
        "        detections.append(list(coord) + [conf] + [cls])\n",
        "    if not labels:\n",
        "        labels.append([d['annotations'][0][\"category_id\"]] + d['annotations'][0][\"bbox\"])\n",
        "    # If detections are empty, add a single entry to \"null\" category\n",
        "    if not detections:\n",
        "        detections.append([0, 0, 0, 0, 0, 8])\n",
        "    confusion_matrix += conf_matrix_calc(np.array(labels), np.array(detections), n_classes, d[\"file_name\"], conf_thresh=CONF_THRESH, iou_thresh=0.5)\n",
        "matrix_indexes = my_dataset_metadata_test.thing_classes + [\"background\"]\n",
        "pd.DataFrame(confusion_matrix, columns=matrix_indexes, index=matrix_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGZiXi4nULz0"
      },
      "outputs": [],
      "source": [
        "#Adapted from stackoverflow response (Griffo, 2012)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have the following code to generate the DataFrame and matrix_indexes\n",
        "metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "matrix_indexes = metadata.thing_classes + [\"background\"]\n",
        "confusion_df = pd.DataFrame(confusion_matrix, columns=matrix_indexes, index=matrix_indexes)\n",
        "\n",
        "# Create a heatmap for the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.set(font_scale=1.2)  # Adjust font size if needed\n",
        "sns.heatmap(confusion_df, annot=True, fmt=\"g\", cmap=\"Blues\", cbar=True, linewidths=0.5)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9bgCPLN6zm8"
      },
      "outputs": [],
      "source": [
        "#After normalisation\n",
        "confusion_matrix_percentage = confusion_df.div(confusion_df.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Create a heatmap for the normalized confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.set(font_scale=1.2)  # Adjust font size if needed\n",
        "sns.heatmap(confusion_matrix_percentage, annot=True, fmt=\".1f\", cmap=\"Blues\", cbar=True, linewidths=0.5)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Normalized Confusion Matrix (%)')\n",
        "plt.show() #powerponint detailing issues with photos and how to improve it,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liENWJfV8nPI"
      },
      "source": [
        "# Evaluate model AP, Presence-based accuracy..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGdHlDqNH_jU"
      },
      "source": [
        "## COCO Evaluation\n",
        "###Adapted from cocoeval source code (Lin et al., 2014)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BsfZNpzJJ8t"
      },
      "outputs": [],
      "source": [
        "###Adapted from cocoeval source code (Lin et al., 2014)\n",
        "#Class: MaskUtil\n",
        "__author__ = 'tsungyi'\n",
        "\n",
        "import pycocotools._mask as _mask\n",
        "\n",
        "# Interface for manipulating masks stored in RLE format.\n",
        "#\n",
        "# RLE is a simple yet efficient format for storing binary masks. RLE\n",
        "# first divides a vector (or vectorized image) into a series of piecewise\n",
        "# constant regions and then for each piece simply stores the length of\n",
        "# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n",
        "# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n",
        "# (note that the odd counts are always the numbers of zeros). Instead of\n",
        "# storing the counts directly, additional compression is achieved with a\n",
        "# variable bitrate representation based on a common scheme called LEB128.\n",
        "#\n",
        "# Compression is greatest given large piecewise constant regions.\n",
        "# Specifically, the size of the RLE is proportional to the number of\n",
        "# *boundaries* in M (or for an image the number of boundaries in the y\n",
        "# direction). Assuming fairly simple shapes, the RLE representation is\n",
        "# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n",
        "# is substantially lower, especially for large simple objects (large n).\n",
        "#\n",
        "# Many common operations on masks can be computed directly using the RLE\n",
        "# (without need for decoding). This includes computations such as area,\n",
        "# union, intersection, etc. All of these operations are linear in the\n",
        "# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n",
        "# of the object. Computing these operations on the original mask is O(n).\n",
        "# Thus, using the RLE can result in substantial computational savings.\n",
        "#\n",
        "# The following API functions are defined:\n",
        "#  encode         - Encode binary masks using RLE.\n",
        "#  decode         - Decode binary masks encoded via RLE.\n",
        "#  merge          - Compute union or intersection of encoded masks.\n",
        "#  iou            - Compute intersection over union between masks.\n",
        "#  area           - Compute area of encoded masks.\n",
        "#  toBbox         - Get bounding boxes surrounding encoded masks.\n",
        "#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n",
        "#\n",
        "# Usage:\n",
        "#  Rs     = encode( masks )\n",
        "#  masks  = decode( Rs )\n",
        "#  R      = merge( Rs, intersect=false )\n",
        "#  o      = iou( dt, gt, iscrowd )\n",
        "#  a      = area( Rs )\n",
        "#  bbs    = toBbox( Rs )\n",
        "#  Rs     = frPyObjects( [pyObjects], h, w )\n",
        "#\n",
        "# In the API the following formats are used:\n",
        "#  Rs      - [dict] Run-length encoding of binary masks\n",
        "#  R       - dict Run-length encoding of binary mask\n",
        "#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n",
        "#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n",
        "#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n",
        "#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n",
        "#  dt,gt   - May be either bounding boxes or encoded masks\n",
        "# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n",
        "#\n",
        "# Finally, a note about the intersection over union (iou) computation.\n",
        "# The standard iou of a ground truth (gt) and detected (dt) object is\n",
        "#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n",
        "# For \"crowd\" regions, we use a modified criteria. If a gt object is\n",
        "# marked as \"iscrowd\", we allow a dt to match any subregion of the gt.\n",
        "# Choosing gt' in the crowd gt that best matches the dt can be done using\n",
        "# gt'=intersect(dt,gt). Since by definition union(gt',dt)=dt, computing\n",
        "#  iou(gt,dt,iscrowd) = iou(gt',dt) = area(intersect(gt,dt)) / area(dt)\n",
        "# For crowd gt regions we use this modified criteria above for the iou.\n",
        "#\n",
        "# To compile run \"python setup.py build_ext --inplace\"\n",
        "# Please do not contact us for help with compiling.\n",
        "#\n",
        "# Microsoft COCO Toolbox.      version 2.0\n",
        "# Data, paper, and tutorials available at:  http://mscoco.org/\n",
        "# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n",
        "# Licensed under the Simplified BSD License [see coco/license.txt]\n",
        "class maskUtil:\n",
        "  iou         = _mask.iou\n",
        "  merge       = _mask.merge\n",
        "  frPyObjects = _mask.frPyObjects\n",
        "\n",
        "  def encode(bimask):\n",
        "      if len(bimask.shape) == 3:\n",
        "          return _mask.encode(bimask)\n",
        "      elif len(bimask.shape) == 2:\n",
        "          h, w = bimask.shape\n",
        "          return _mask.encode(bimask.reshape((h, w, 1), order='F'))[0]\n",
        "\n",
        "  def decode(rleObjs):\n",
        "      if type(rleObjs) == list:\n",
        "          return _mask.decode(rleObjs)\n",
        "      else:\n",
        "          return _mask.decode([rleObjs])[:,:,0]\n",
        "\n",
        "  def area(rleObjs):\n",
        "      if type(rleObjs) == list:\n",
        "          return _mask.area(rleObjs)\n",
        "      else:\n",
        "          return _mask.area([rleObjs])[0]\n",
        "\n",
        "  def toBbox(rleObjs):\n",
        "      if type(rleObjs) == list:\n",
        "          return _mask.toBbox(rleObjs)\n",
        "      else:\n",
        "          return _mask.toBbox([rleObjs])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyv8wbNqJI51"
      },
      "outputs": [],
      "source": [
        "###Adapted from cocoeval source code (Lin et al., 2014)\n",
        "#Evaluator.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "import datetime\n",
        "import logging\n",
        "import time\n",
        "from collections import OrderedDict, abc\n",
        "from contextlib import ExitStack, contextmanager\n",
        "from typing import List, Union\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from detectron2.utils.comm import get_world_size, is_main_process\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "\n",
        "\n",
        "class DatasetEvaluator1:\n",
        "    \"\"\"\n",
        "    Base class for a dataset evaluator.\n",
        "\n",
        "    The function :func:`inference_on_dataset` runs the model over\n",
        "    all samples in the dataset, and have a DatasetEvaluator to process the inputs/outputs.\n",
        "\n",
        "    This class will accumulate information of the inputs/outputs (by :meth:`process`),\n",
        "    and produce evaluation results in the end (by :meth:`evaluate`).\n",
        "    \"\"\"\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Preparation for a new round of evaluation.\n",
        "        Should be called before starting a round of evaluation.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def process(self, inputs, outputs):\n",
        "        \"\"\"\n",
        "        Process the pair of inputs and outputs.\n",
        "        If they contain batches, the pairs can be consumed one-by-one using `zip`:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            for input_, output in zip(inputs, outputs):\n",
        "                # do evaluation on single input/output pair\n",
        "                ...\n",
        "\n",
        "        Args:\n",
        "            inputs (list): the inputs that's used to call the model.\n",
        "            outputs (list): the return value of `model(inputs)`\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluate/summarize the performance, after processing all input/output pairs.\n",
        "\n",
        "        Returns:\n",
        "            dict:\n",
        "                A new evaluator class can return a dict of arbitrary format\n",
        "                as long as the user can process the results.\n",
        "                In our train_net.py, we expect the following format:\n",
        "\n",
        "                * key: the name of the task (e.g., bbox)\n",
        "                * value: a dict of {metric name: score}, e.g.: {\"AP50\": 80}\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class DatasetEvaluators(DatasetEvaluator1):\n",
        "    \"\"\"\n",
        "    Wrapper class to combine multiple :class:`DatasetEvaluator` instances.\n",
        "\n",
        "    This class dispatches every evaluation call to\n",
        "    all of its :class:`DatasetEvaluator`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, evaluators):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            evaluators (list): the evaluators to combine.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._evaluators = evaluators\n",
        "\n",
        "    def reset(self):\n",
        "        for evaluator in self._evaluators:\n",
        "            evaluator.reset()\n",
        "\n",
        "    def process(self, inputs, outputs):\n",
        "        for evaluator in self._evaluators:\n",
        "            evaluator.process(inputs, outputs)\n",
        "\n",
        "    def evaluate(self):\n",
        "        results = OrderedDict()\n",
        "        for evaluator in self._evaluators:\n",
        "            result = evaluator.evaluate()\n",
        "            if is_main_process() and result is not None:\n",
        "                for k, v in result.items():\n",
        "                    assert (\n",
        "                        k not in results\n",
        "                    ), \"Different evaluators produce results with the same key {}\".format(k)\n",
        "                    results[k] = v\n",
        "        return results\n",
        "\n",
        "\n",
        "def inference_on_dataset(\n",
        "    model, data_loader, evaluator: Union[DatasetEvaluator1, List[DatasetEvaluator1], None]\n",
        "):\n",
        "    \"\"\"\n",
        "    Run model on the data_loader and evaluate the metrics with evaluator.\n",
        "    Also benchmark the inference speed of `model.__call__` accurately.\n",
        "    The model will be used in eval mode.\n",
        "\n",
        "    Args:\n",
        "        model (callable): a callable which takes an object from\n",
        "            `data_loader` and returns some outputs.\n",
        "\n",
        "            If it's an nn.Module, it will be temporarily set to `eval` mode.\n",
        "            If you wish to evaluate a model in `training` mode instead, you can\n",
        "            wrap the given model and override its behavior of `.eval()` and `.train()`.\n",
        "        data_loader: an iterable object with a length.\n",
        "            The elements it generates will be the inputs to the model.\n",
        "        evaluator: the evaluator(s) to run. Use `None` if you only want to benchmark,\n",
        "            but don't want to do any evaluation.\n",
        "\n",
        "    Returns:\n",
        "        The return value of `evaluator.evaluate()`\n",
        "    \"\"\"\n",
        "    num_devices = get_world_size()\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Start inference on {} batches\".format(len(data_loader)))\n",
        "\n",
        "    total = len(data_loader)  # inference data loader must have a fixed length\n",
        "    if evaluator is None:\n",
        "        # create a no-op evaluator\n",
        "        evaluator = DatasetEvaluators([])\n",
        "    if isinstance(evaluator, abc.MutableSequence):\n",
        "        evaluator = DatasetEvaluators(evaluator)\n",
        "    evaluator.reset()\n",
        "\n",
        "    num_warmup = min(5, total - 1)\n",
        "    start_time = time.perf_counter()\n",
        "    total_data_time = 0\n",
        "    total_compute_time = 0\n",
        "    total_eval_time = 0\n",
        "    with ExitStack() as stack:\n",
        "        if isinstance(model, nn.Module):\n",
        "            stack.enter_context(inference_context(model))\n",
        "        stack.enter_context(torch.no_grad())\n",
        "\n",
        "        start_data_time = time.perf_counter()\n",
        "        for idx, inputs in enumerate(data_loader):\n",
        "            total_data_time += time.perf_counter() - start_data_time\n",
        "            if idx == num_warmup:\n",
        "                start_time = time.perf_counter()\n",
        "                total_data_time = 0\n",
        "                total_compute_time = 0\n",
        "                total_eval_time = 0\n",
        "\n",
        "            start_compute_time = time.perf_counter()\n",
        "            outputs = model(inputs)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            total_compute_time += time.perf_counter() - start_compute_time\n",
        "\n",
        "            start_eval_time = time.perf_counter()\n",
        "            evaluator.process(inputs, outputs)\n",
        "            total_eval_time += time.perf_counter() - start_eval_time\n",
        "\n",
        "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
        "            data_seconds_per_iter = total_data_time / iters_after_start\n",
        "            compute_seconds_per_iter = total_compute_time / iters_after_start\n",
        "            eval_seconds_per_iter = total_eval_time / iters_after_start\n",
        "            total_seconds_per_iter = (time.perf_counter() - start_time) / iters_after_start\n",
        "            if idx >= num_warmup * 2 or compute_seconds_per_iter > 5:\n",
        "                eta = datetime.timedelta(seconds=int(total_seconds_per_iter * (total - idx - 1)))\n",
        "                log_every_n_seconds(\n",
        "                    logging.INFO,\n",
        "                    (\n",
        "                        f\"Inference done {idx + 1}/{total}. \"\n",
        "                        f\"Dataloading: {data_seconds_per_iter:.4f} s/iter. \"\n",
        "                        f\"Inference: {compute_seconds_per_iter:.4f} s/iter. \"\n",
        "                        f\"Eval: {eval_seconds_per_iter:.4f} s/iter. \"\n",
        "                        f\"Total: {total_seconds_per_iter:.4f} s/iter. \"\n",
        "                        f\"ETA={eta}\"\n",
        "                    ),\n",
        "                    n=5,\n",
        "                )\n",
        "            start_data_time = time.perf_counter()\n",
        "\n",
        "    # Measure the time only for this worker (before the synchronization barrier)\n",
        "    total_time = time.perf_counter() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=total_time))\n",
        "    # NOTE this format is parsed by grep\n",
        "    logger.info(\n",
        "        \"Total inference time: {} ({:.6f} s / iter per device, on {} devices)\".format(\n",
        "            total_time_str, total_time / (total - num_warmup), num_devices\n",
        "        )\n",
        "    )\n",
        "    total_compute_time_str = str(datetime.timedelta(seconds=int(total_compute_time)))\n",
        "    logger.info(\n",
        "        \"Total inference pure compute time: {} ({:.6f} s / iter per device, on {} devices)\".format(\n",
        "            total_compute_time_str, total_compute_time / (total - num_warmup), num_devices\n",
        "        )\n",
        "    )\n",
        "\n",
        "    results = evaluator.evaluate()\n",
        "    # An evaluator may return None when not in main process.\n",
        "    # Replace it by an empty dict instead to make it easier for downstream code to handle\n",
        "    if results is None:\n",
        "        results = {}\n",
        "    return results\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def inference_context(model):\n",
        "    \"\"\"\n",
        "    A context where the model is temporarily changed to eval mode,\n",
        "    and restored to previous mode afterwards.\n",
        "\n",
        "    Args:\n",
        "        model: a torch Module\n",
        "    \"\"\"\n",
        "    training_mode = model.training\n",
        "    model.eval()\n",
        "    yield\n",
        "    model.train(training_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I59TjPaFRA7V"
      },
      "outputs": [],
      "source": [
        "###Adapted from cocoeval source code (Lin et al., 2014)\n",
        "#cocoeval.py\n",
        "__author__ = 'tsungyi'\n",
        "\n",
        "import numpy as np\n",
        "import datetime\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "\n",
        "class COCOeval1:\n",
        "    # Interface for evaluating detection on the Microsoft COCO dataset.\n",
        "    #\n",
        "    # The usage for CocoEval is as follows:\n",
        "    #  cocoGt=..., cocoDt=...       # load dataset and results\n",
        "    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n",
        "    #  E.params.recThrs = ...;      # set parameters as desired\n",
        "    #  E.evaluate();                # run per image evaluation\n",
        "    #  E.accumulate();              # accumulate per image results\n",
        "    #  E.summarize();               # display summary metrics of results\n",
        "    # For example usage see evalDemo.m and http://mscoco.org/.\n",
        "    #\n",
        "    # The evaluation parameters are as follows (defaults in brackets):\n",
        "    #  imgIds     - [all] N img ids to use for evaluation\n",
        "    #  catIds     - [all] K cat ids to use for evaluation\n",
        "    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n",
        "    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n",
        "    #  areaRng    - [...] A=4 object area ranges for evaluation\n",
        "    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n",
        "    #  iouType    - ['segm'] set iouType to 'segm', 'bbox' or 'keypoints'\n",
        "    #  iouType replaced the now DEPRECATED useSegm parameter.\n",
        "    #  useCats    - [1] if true use category labels for evaluation\n",
        "    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n",
        "    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n",
        "    #\n",
        "    # evaluate(): evaluates detections on every image and every category and\n",
        "    # concats the results into the \"evalImgs\" with fields:\n",
        "    #  dtIds      - [1xD] id for each of the D detections (dt)\n",
        "    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n",
        "    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n",
        "    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n",
        "    #  dtScores   - [1xD] confidence of each dt\n",
        "    #  gtIgnore   - [1xG] ignore flag for each gt\n",
        "    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n",
        "    #\n",
        "    # accumulate(): accumulates the per-image, per-category evaluation\n",
        "    # results in \"evalImgs\" into the dictionary \"eval\" with fields:\n",
        "    #  params     - parameters used for evaluation\n",
        "    #  date       - date evaluation was performed\n",
        "    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n",
        "    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n",
        "    #  recall     - [TxKxAxM] max recall for every evaluation setting\n",
        "    # Note: precision and recall==-1 for settings with no gt objects.\n",
        "    #\n",
        "    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n",
        "    #\n",
        "    # Microsoft COCO Toolbox.      version 2.0\n",
        "    # Data, paper, and tutorials available at:  http://mscoco.org/\n",
        "    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n",
        "    # Licensed under the Simplified BSD License [see coco/license.txt]\n",
        "    def __init__(self, cocoGt=None, cocoDt=None, iouType='segm', class_names=None):\n",
        "        '''\n",
        "        Initialize CocoEval using coco APIs for gt and dt\n",
        "        :param cocoGt: coco object with ground truth annotations\n",
        "        :param cocoDt: coco object with detection results\n",
        "        :return: None\n",
        "        '''\n",
        "        if not iouType:\n",
        "            print('iouType not specified. use default iouType segm')\n",
        "        self.cocoGt   = cocoGt              # ground truth COCO API\n",
        "        self.cocoDt   = cocoDt              # detections COCO API\n",
        "        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n",
        "        self.eval     = {}                  # accumulated evaluation results\n",
        "        self._gts = defaultdict(list)       # gt for evaluation\n",
        "        self._dts = defaultdict(list)       # dt for evaluation\n",
        "        self.params = Params(iouType=iouType) # parameters\n",
        "        self._paramsEval = {}               # parameters for evaluation\n",
        "        self.stats = []                     # result summarization\n",
        "        self.ious = {}                      # ious between all gts and dts\n",
        "        self.class_names = class_names\n",
        "        if not cocoGt is None:\n",
        "            self.params.imgIds = sorted(cocoGt.getImgIds())\n",
        "            self.params.catIds = sorted(cocoGt.getCatIds())\n",
        "\n",
        "\n",
        "    def _prepare(self):\n",
        "        '''\n",
        "        Prepare ._gts and ._dts for evaluation based on params\n",
        "        :return: None\n",
        "        '''\n",
        "        def _toMask(anns, coco):\n",
        "            # modify ann['segmentation'] by reference\n",
        "            for ann in anns:\n",
        "                rle = coco.annToRLE(ann)\n",
        "                ann['segmentation'] = rle\n",
        "        p = self.params\n",
        "        if p.useCats:\n",
        "            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n",
        "            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n",
        "        else:\n",
        "            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n",
        "            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n",
        "\n",
        "        # convert ground truth to mask if iouType == 'segm'\n",
        "        if p.iouType == 'segm':\n",
        "            _toMask(gts, self.cocoGt)\n",
        "            _toMask(dts, self.cocoDt)\n",
        "        # set ignore flag\n",
        "        for gt in gts:\n",
        "            gt['ignore'] = gt['ignore'] if 'ignore' in gt else 0\n",
        "            gt['ignore'] = 'iscrowd' in gt and gt['iscrowd']\n",
        "            if p.iouType == 'keypoints':\n",
        "                gt['ignore'] = (gt['num_keypoints'] == 0) or gt['ignore']\n",
        "        self._gts = defaultdict(list)       # gt for evaluation\n",
        "        self._dts = defaultdict(list)       # dt for evaluation\n",
        "        for gt in gts:\n",
        "            self._gts[gt['image_id'], gt['category_id']].append(gt)\n",
        "        for dt in dts:\n",
        "            self._dts[dt['image_id'], dt['category_id']].append(dt)\n",
        "        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n",
        "        self.eval     = {}                  # accumulated evaluation results\n",
        "\n",
        "    def evaluate(self):\n",
        "        '''\n",
        "        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "        :return: None\n",
        "        '''\n",
        "        tic = time.time()\n",
        "        print('Running per image evaluation...')\n",
        "        p = self.params\n",
        "        # add backward compatibility if useSegm is specified in params\n",
        "        if not p.useSegm is None:\n",
        "            p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "            print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "        print('Evaluate annotation type *{}*'.format(p.iouType))\n",
        "        p.imgIds = list(np.unique(p.imgIds))\n",
        "        if p.useCats:\n",
        "            p.catIds = list(np.unique(p.catIds))\n",
        "        p.maxDets = sorted(p.maxDets)\n",
        "        self.params=p\n",
        "\n",
        "        self._prepare()\n",
        "        # loop through images, area range, max detection number\n",
        "        catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "        if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "            computeIoU = self.computeIoU\n",
        "        elif p.iouType == 'keypoints':\n",
        "            computeIoU = self.computeOks\n",
        "        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n",
        "                        for imgId in p.imgIds\n",
        "                        for catId in catIds}\n",
        "\n",
        "        evaluateImg = self.evaluateImg\n",
        "        maxDet = p.maxDets[-1]\n",
        "        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "                 for catId in catIds\n",
        "                 for areaRng in p.areaRng\n",
        "                 for imgId in p.imgIds\n",
        "             ]\n",
        "        self._paramsEval = copy.deepcopy(self.params)\n",
        "        toc = time.time()\n",
        "        print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
        "\n",
        "    def computeIoU(self, imgId, catId):\n",
        "        p = self.params\n",
        "        if p.useCats:\n",
        "            gt = self._gts[imgId,catId]\n",
        "            dt = self._dts[imgId,catId]\n",
        "        else:\n",
        "            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n",
        "            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n",
        "        if len(gt) == 0 and len(dt) ==0:\n",
        "            return []\n",
        "        inds = np.argsort([-d['score'] for d in dt], kind='mergesort')\n",
        "        dt = [dt[i] for i in inds]\n",
        "        if len(dt) > p.maxDets[-1]:\n",
        "            dt=dt[0:p.maxDets[-1]]\n",
        "\n",
        "        if p.iouType == 'segm':\n",
        "            g = [g['segmentation'] for g in gt]\n",
        "            d = [d['segmentation'] for d in dt]\n",
        "        elif p.iouType == 'bbox':\n",
        "            g = [g['bbox'] for g in gt]\n",
        "            d = [d['bbox'] for d in dt]\n",
        "        else:\n",
        "            raise Exception('unknown iouType for iou computation')\n",
        "        # compute iou between each dt and gt region\n",
        "        iscrowd = [int(o['iscrowd']) for o in gt]\n",
        "        ious = maskUtil.iou(d,g,iscrowd)\n",
        "        return ious\n",
        "\n",
        "    def computeOks(self, imgId, catId):\n",
        "        p = self.params\n",
        "        # dimention here should be Nxm\n",
        "        gts = self._gts[imgId, catId]\n",
        "        dts = self._dts[imgId, catId]\n",
        "        inds = np.argsort([-d['score'] for d in dts], kind='mergesort')\n",
        "        dts = [dts[i] for i in inds]\n",
        "        if len(dts) > p.maxDets[-1]:\n",
        "            dts = dts[0:p.maxDets[-1]]\n",
        "        # if len(gts) == 0 and len(dts) == 0:\n",
        "        if len(gts) == 0 or len(dts) == 0:\n",
        "            return []\n",
        "        ious = np.zeros((len(dts), len(gts)))\n",
        "        sigmas = p.kpt_oks_sigmas\n",
        "        vars = (sigmas * 2)**2\n",
        "        k = len(sigmas)\n",
        "        # compute oks between each detection and ground truth object\n",
        "        for j, gt in enumerate(gts):\n",
        "            # create bounds for ignore regions(double the gt bbox)\n",
        "            g = np.array(gt['keypoints'])\n",
        "            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n",
        "            k1 = np.count_nonzero(vg > 0)\n",
        "            bb = gt['bbox']\n",
        "            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n",
        "            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n",
        "            for i, dt in enumerate(dts):\n",
        "                d = np.array(dt['keypoints'])\n",
        "                xd = d[0::3]; yd = d[1::3]\n",
        "                if k1>0:\n",
        "                    # measure the per-keypoint distance if keypoints visible\n",
        "                    dx = xd - xg\n",
        "                    dy = yd - yg\n",
        "                else:\n",
        "                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n",
        "                    z = np.zeros((k))\n",
        "                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n",
        "                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n",
        "                e = (dx**2 + dy**2) / vars / (gt['area']+np.spacing(1)) / 2\n",
        "                if k1 > 0:\n",
        "                    e=e[vg > 0]\n",
        "                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n",
        "        return ious\n",
        "\n",
        "    def evaluateImg(self, imgId, catId, aRng, maxDet):\n",
        "        '''\n",
        "        perform evaluation for single category and image\n",
        "        :return: dict (single image results)\n",
        "        '''\n",
        "        p = self.params\n",
        "        if p.useCats:\n",
        "            gt = self._gts[imgId,catId]\n",
        "            dt = self._dts[imgId,catId]\n",
        "        else:\n",
        "            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n",
        "            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n",
        "        if len(gt) == 0 and len(dt) ==0:\n",
        "            return None\n",
        "\n",
        "        for g in gt:\n",
        "            if g['ignore'] or (g['area']<aRng[0] or g['area']>aRng[1]):\n",
        "                g['_ignore'] = 1\n",
        "            else:\n",
        "                g['_ignore'] = 0\n",
        "\n",
        "        # sort dt highest score first, sort gt ignore last\n",
        "        gtind = np.argsort([g['_ignore'] for g in gt], kind='mergesort')\n",
        "        gt = [gt[i] for i in gtind]\n",
        "        dtind = np.argsort([-d['score'] for d in dt], kind='mergesort')\n",
        "        dt = [dt[i] for i in dtind[0:maxDet]]\n",
        "        iscrowd = [int(o['iscrowd']) for o in gt]\n",
        "        # load computed ious\n",
        "        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n",
        "        T = len(p.iouThrs)\n",
        "        G = len(gt)\n",
        "        D = len(dt)\n",
        "        gtm  = np.zeros((T,G))\n",
        "        dtm  = np.zeros((T,D))\n",
        "        gtIg = np.array([g['_ignore'] for g in gt])\n",
        "        dtIg = np.zeros((T,D))\n",
        "\n",
        "        # Set of unique categories in ground truth and detections ##NEW\n",
        "        gt_categories = set([g['category_id'] for g in gt])\n",
        "        dt_categories = set([d['category_id'] for d in dt])\n",
        "\n",
        "        if not len(ious)==0:\n",
        "            for tind, t in enumerate(p.iouThrs):\n",
        "                for dind, d in enumerate(dt):\n",
        "                    # information about best match so far (m=-1 -> unmatched)\n",
        "                    iou = min([t,1-1e-10])\n",
        "                    m   = -1\n",
        "                    for gind, g in enumerate(gt):\n",
        "                        # if this gt already matched, and not a crowd, continue\n",
        "                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n",
        "                            continue\n",
        "                        # if dt matched to reg gt, and on ignore gt, stop\n",
        "                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n",
        "                            break\n",
        "                        # continue to next gt unless better match made\n",
        "                        if ious[dind,gind] < iou:\n",
        "                            continue\n",
        "                        # if match successful and best so far, store appropriately\n",
        "                        iou=ious[dind,gind]\n",
        "                        m=gind\n",
        "                    # if match made store id of match for both dt and gt\n",
        "                    if m ==-1:\n",
        "                        continue\n",
        "                    dtIg[tind,dind] = gtIg[m]\n",
        "                    dtm[tind,dind]  = gt[m]['id']\n",
        "                    gtm[tind,m]     = d['id']\n",
        "        # set unmatched detections outside of area range to ignore\n",
        "        a = np.array([d['area']<aRng[0] or d['area']>aRng[1] for d in dt]).reshape((1, len(dt)))\n",
        "        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n",
        "        # store results for given image and category\n",
        "        return {\n",
        "                'image_id':     imgId,\n",
        "                'category_id':  catId,\n",
        "                'aRng':         aRng,\n",
        "                'maxDet':       maxDet,\n",
        "                'dtIds':        [d['id'] for d in dt],\n",
        "                'gtIds':        [g['id'] for g in gt],\n",
        "                'dtMatches':    dtm,\n",
        "                'gtMatches':    gtm,\n",
        "                'dtScores':     [d['score'] for d in dt],\n",
        "                'gtIgnore':     gtIg,\n",
        "                'dtIgnore':     dtIg,\n",
        "                'gtCategories': gt_categories, ##NEW\n",
        "                'dtCategories': dt_categories ##NEW\n",
        "            }\n",
        "\n",
        "    def accumulate(self, p = None):\n",
        "        '''\n",
        "        Accumulate per image evaluation results and store the result in self.eval\n",
        "        :param p: input params for evaluation\n",
        "        :return: None\n",
        "        '''\n",
        "        print('Accumulating evaluation results...')\n",
        "        tic = time.time()\n",
        "        if not self.evalImgs:\n",
        "            print('Please run evaluate() first')\n",
        "        # allows input customized parameters\n",
        "        if p is None:\n",
        "            p = self.params\n",
        "        p.catIds = p.catIds if p.useCats == 1 else [-1]\n",
        "        T           = len(p.iouThrs)\n",
        "        R           = len(p.recThrs)\n",
        "        K           = len(p.catIds) if p.useCats else 1\n",
        "        A           = len(p.areaRng)\n",
        "        M           = len(p.maxDets)\n",
        "        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n",
        "        recall      = -np.ones((T,K,A,M))\n",
        "        scores      = -np.ones((T,R,K,A,M))\n",
        "\n",
        "        # Initialize arrays for presence-based metrics ##NEW\n",
        "        presence_based_precision = -np.ones(K)\n",
        "        presence_based_recall = -np.ones(K)\n",
        "        presence_based_accuracy = -np.ones(K)\n",
        "\n",
        "        # create dictionary for future indexing\n",
        "        _pe = self._paramsEval\n",
        "        catIds = _pe.catIds if _pe.useCats else [-1]\n",
        "        setK = set(catIds)\n",
        "        setA = set(map(tuple, _pe.areaRng))\n",
        "        setM = set(_pe.maxDets)\n",
        "        setI = set(_pe.imgIds)\n",
        "        # get inds to evaluate\n",
        "        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n",
        "        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n",
        "        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n",
        "        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n",
        "        I0 = len(_pe.imgIds)\n",
        "        A0 = len(_pe.areaRng)\n",
        "\n",
        "        #total presence based metrics\n",
        "        total_tp = total_fp = total_fn = 0\n",
        "        presence_based_confusion_matrix = np.zeros([len(catIds) + 1, len(catIds) + 1])\n",
        "        background_index = len(p.catIds)\n",
        "\n",
        "        # retrieve E at each category, area range, and max number of detections\n",
        "        for k_idx, catId in enumerate(p.catIds):  # Iterate over categories\n",
        "          p_tp = p_fp = p_fn = 0\n",
        "\n",
        "          valid_evalImgs = [e for e in self.evalImgs if e is not None]\n",
        "\n",
        "          for imgId in p.imgIds:  # Iterate over images\n",
        "              # Use the filtered list (valid_evalImgs) for iteration\n",
        "              E = next((e for e in valid_evalImgs if e['image_id'] == imgId and e['category_id'] == catId), None)\n",
        "              if E is None:\n",
        "                  continue\n",
        "\n",
        "              gt_present = E['gtCategories']\n",
        "              dt_present = E['dtCategories']\n",
        "\n",
        "              # Process the TP, FP, FN logic here for each image\n",
        "              if gt_present == dt_present:\n",
        "                  category_id = next(iter(dt_present), background_index)\n",
        "                  presence_based_confusion_matrix[category_id, category_id] += 1\n",
        "                  p_tp += 1\n",
        "              else:\n",
        "                  for category_id in dt_present - gt_present:\n",
        "                      presence_based_confusion_matrix[category_id, background_index] += 1\n",
        "                      p_fp += 1\n",
        "                  for category_id in gt_present - dt_present:\n",
        "                      presence_based_confusion_matrix[background_index, category_id] += 1\n",
        "                      p_fn += 1\n",
        "                              # Update the metrics only if calculations are applicable\n",
        "          if p_tp + p_fp > 0 or p_tp + p_fn > 0:\n",
        "              presence_based_precision[k_idx] = p_tp / (p_tp + p_fp) if (p_tp + p_fp) > 0 else -1\n",
        "              presence_based_recall[k_idx] = p_tp / (p_tp + p_fn) if (p_tp + p_fn) > 0 else -1\n",
        "              presence_based_accuracy[k_idx] = p_tp / (p_tp + p_fp + p_fn) if (p_tp + p_fp + p_fn) > 0 else -1\n",
        "\n",
        "          total_tp += p_tp\n",
        "          total_fp += p_fp\n",
        "          total_fn += p_fn\n",
        "\n",
        "        for k, k0 in enumerate(k_list): # for every category\n",
        "            Nk = k0*A0*I0\n",
        "            for a, a0 in enumerate(a_list): #for every area range\n",
        "                Na = a0*I0\n",
        "                for m, maxDet in enumerate(m_list): #for every maximum nuymber of detections\n",
        "                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n",
        "                    E = [e for e in E if not e is None]\n",
        "                    if len(E) == 0:\n",
        "                        continue\n",
        "                    dtScores = np.concatenate([e['dtScores'][0:maxDet] for e in E])\n",
        "\n",
        "                    # different sorting method generates slightly different results.\n",
        "                    # mergesort is used to be consistent as Matlab implementation.\n",
        "                    inds = np.argsort(-dtScores, kind='mergesort')\n",
        "                    dtScoresSorted = dtScores[inds]\n",
        "\n",
        "                    dtm  = np.concatenate([e['dtMatches'][:,0:maxDet] for e in E], axis=1)[:,inds]\n",
        "                    dtIg = np.concatenate([e['dtIgnore'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n",
        "                    gtIg = np.concatenate([e['gtIgnore'] for e in E])\n",
        "                    npig = np.count_nonzero(gtIg==0 )\n",
        "\n",
        "                    if npig == 0:\n",
        "                        continue\n",
        "                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n",
        "                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n",
        "                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=float)\n",
        "                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=float)\n",
        "                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n",
        "                        tp = np.array(tp)\n",
        "                        fp = np.array(fp)\n",
        "                        nd = len(tp)\n",
        "                        rc = tp / npig\n",
        "                        pr = tp / (fp+tp+np.spacing(1))\n",
        "                        q  = np.zeros((R,))\n",
        "                        ss = np.zeros((R,))\n",
        "\n",
        "                        if nd:\n",
        "                            recall[t,k,a,m] = rc[-1]\n",
        "                        else:\n",
        "                            recall[t,k,a,m] = 0\n",
        "\n",
        "                        # numpy is slow without cython optimization for accessing elements\n",
        "                        # use python array gets significant speed improvement\n",
        "                        pr = pr.tolist(); q = q.tolist()\n",
        "\n",
        "                        for i in range(nd-1, 0, -1):\n",
        "                            if pr[i] > pr[i-1]:\n",
        "                                pr[i-1] = pr[i]\n",
        "\n",
        "                        inds = np.searchsorted(rc, p.recThrs, side='left')\n",
        "                        try:\n",
        "                            for ri, pi in enumerate(inds):\n",
        "                                q[ri] = pr[pi]\n",
        "                                ss[ri] = dtScoresSorted[pi]\n",
        "                        except:\n",
        "                            pass\n",
        "                        precision[t,:,k,a,m] = np.array(q)\n",
        "                        scores[t,:,k,a,m] = np.array(ss)\n",
        "\n",
        "        # Calculate total metrics outside the loop over categories but inside the loop for areaRng and maxDets\n",
        "        if total_tp + total_fp > 0 or total_tp + total_fn > 0:\n",
        "            total_presence_based_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else -1\n",
        "            total_presence_based_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else -1\n",
        "            total_presence_based_accuracy = total_tp / (total_tp + total_fp + total_fn) if (total_tp + total_fp + total_fn) > 0 else -1\n",
        "        else:\n",
        "            total_presence_based_precision = total_presence_based_recall = total_presence_based_accuracy = -1\n",
        "\n",
        "        self.eval = {\n",
        "            'params': p,\n",
        "            'counts': [T, R, K, A, M],\n",
        "            'date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'precision': precision,\n",
        "            'recall':   recall,\n",
        "            'scores': scores,\n",
        "            'presence_based_precision' : presence_based_precision,\n",
        "            'presence_based_recall' : presence_based_recall,\n",
        "            'presence_based_accuracy' : presence_based_accuracy,\n",
        "            'total_presence_based_precision' : total_presence_based_precision,\n",
        "            'total_presence_based_recall' : total_presence_based_recall,\n",
        "            'total_presence_based_accuracy' : total_presence_based_accuracy,\n",
        "            'presence_based_confusion_matrix' : presence_based_confusion_matrix\n",
        "\n",
        "        }\n",
        "        toc = time.time()\n",
        "        print('DONE (t={:0.2f}s).'.format( toc-tic))\n",
        "\n",
        "    def summarize(self):\n",
        "        '''\n",
        "        Compute and display summary metrics for evaluation results.\n",
        "        Note this functin can *only* be applied on the default parameter setting\n",
        "        '''\n",
        "\n",
        "        def _summarizePresenceBasedMetrics(metric, areaRng='all', maxDets=100):\n",
        "\n",
        "            p = self.params\n",
        "            titleStr = metric.replace('_', ' ').capitalize()\n",
        "            if metric == \"presence_based_confusion_matrix\":\n",
        "                print(\"Presence based confusion matrix\")\n",
        "                confusion_df = pd.DataFrame(self.eval[metric], columns=self.class_names + [\"background\"], index = self.class_names + [\"background\"])#After normalisation\n",
        "                confusion_matrix_percentage = confusion_df.div(confusion_df.sum(axis=1), axis=0) * 100\n",
        "\n",
        "                # Create a heatmap for the normalized confusion matrix\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                sns.set(font_scale=1.2)  # Adjust font size if needed\n",
        "                sns.heatmap(confusion_matrix_percentage, annot=True, fmt=\".1f\", cmap=\"Purples\", cbar=True, linewidths=0.5)\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('Actual')\n",
        "                plt.title('Normalized Presence-Based Confusion Matrix (%)')\n",
        "                plt.show() #powerponint detailing issues with photos and how to improve it,\n",
        "\n",
        "                return\n",
        "            # Handling for total metrics\n",
        "            if 'total_' in metric:  # Check if this is a request for total metrics\n",
        "                if metric in self.eval:\n",
        "                    # Assuming the total metrics are stored in a similar [areaIndex, maxDetIndex] format\n",
        "                    total_metric_value = self.eval[metric]\n",
        "                    print(f\"\\n{titleStr}: {total_metric_value:.3f}\")\n",
        "                else:\n",
        "                    print(f\"\\n{titleStr} not available.\")\n",
        "                return\n",
        "\n",
        "            print(f\"\\n{titleStr} per Category:\")\n",
        "\n",
        "            for k, catId in enumerate(p.catIds):\n",
        "                if self.class_names is not None:\n",
        "                  catName = self.class_names[catId]\n",
        "                else:\n",
        "                  catName = catId\n",
        "                metric_value = self.eval[metric][k]\n",
        "                if metric_value >= 0:  # Check if metric was calculated\n",
        "                    print(f\" - {catName}: {metric_value:.3f}\")\n",
        "                else:\n",
        "                    print(f\" - {catName}: N/A\")\n",
        "\n",
        "        def _summarize( ap=1, iouThr=None, areaRng='all', maxDets=100, metric=None):\n",
        "            p = self.params\n",
        "            iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n",
        "            titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n",
        "            typeStr = '(AP)' if ap==1 else '(AR)'\n",
        "            iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n",
        "                if iouThr is None else '{:0.2f}'.format(iouThr)\n",
        "\n",
        "            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
        "            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
        "            if ap == 1:\n",
        "                # dimension of precision: [TxRxKxAxM]\n",
        "                s = self.eval['precision']\n",
        "                # IoU\n",
        "                if iouThr is not None:\n",
        "                    t = np.where(iouThr == p.iouThrs)[0]\n",
        "                    s = s[t]\n",
        "                s = s[:,:,:,aind,mind]\n",
        "            else:\n",
        "                # dimension of recall: [TxKxAxM]\n",
        "                s = self.eval['recall']\n",
        "                if iouThr is not None:\n",
        "                    t = np.where(iouThr == p.iouThrs)[0]\n",
        "                    s = s[t]\n",
        "                s = s[:,:,aind,mind]\n",
        "            if len(s[s>-1])==0:\n",
        "                mean_s = -1\n",
        "            else:\n",
        "                mean_s = np.mean(s[s>-1])\n",
        "            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
        "            return mean_s\n",
        "\n",
        "        def _summarizeDets():\n",
        "            stats = np.zeros((19,))\n",
        "            stats[0] = _summarize(1)\n",
        "            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n",
        "            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n",
        "            stats[3] = _summarize(1, areaRng='small', maxDets=self.params.maxDets[2])\n",
        "            stats[4] = _summarize(1, areaRng='medium', maxDets=self.params.maxDets[2])\n",
        "            stats[5] = _summarize(1, areaRng='large', maxDets=self.params.maxDets[2])\n",
        "            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n",
        "            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n",
        "            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n",
        "            stats[9] = _summarize(0, areaRng='small', maxDets=self.params.maxDets[2])\n",
        "            stats[10] = _summarize(0, areaRng='medium', maxDets=self.params.maxDets[2])\n",
        "            stats[11] = _summarize(0, areaRng='large', maxDets=self.params.maxDets[2])\n",
        "            stats[12] = _summarizePresenceBasedMetrics('presence_based_precision', areaRng='all', maxDets=self.params.maxDets[2])\n",
        "            stats[13] = _summarizePresenceBasedMetrics('total_presence_based_precision', areaRng='all', maxDets=self.params.maxDets[2])\n",
        "            stats[14] = _summarizePresenceBasedMetrics('presence_based_recall', areaRng='all', maxDets=self.params.maxDets[2])\n",
        "            stats[15] = _summarizePresenceBasedMetrics('total_presence_based_recall', areaRng='all', maxDets=self.params.maxDets[2])\n",
        "            stats[16] = _summarizePresenceBasedMetrics('presence_based_accuracy', areaRng='all', maxDets=self.params.maxDets[2])\n",
        "            stats[17] = _summarizePresenceBasedMetrics('total_presence_based_accuracy', areaRng='all', maxDets=self.params.maxDets[2])\n",
        "            stats[18] = _summarizePresenceBasedMetrics('presence_based_confusion_matrix', areaRng='all', maxDets=self.params.maxDets[2])\n",
        "\n",
        "\n",
        "            return stats\n",
        "\n",
        "        def _summarizeKps():\n",
        "            stats = np.zeros((10,))\n",
        "            stats[0] = _summarize(1, maxDets=20)\n",
        "            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n",
        "            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n",
        "            stats[3] = _summarize(1, maxDets=20, areaRng='medium')\n",
        "            stats[4] = _summarize(1, maxDets=20, areaRng='large')\n",
        "            stats[5] = _summarize(0, maxDets=20)\n",
        "            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n",
        "            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n",
        "            stats[8] = _summarize(0, maxDets=20, areaRng='medium')\n",
        "            stats[9] = _summarize(0, maxDets=20, areaRng='large')\n",
        "            return stats\n",
        "        if not self.eval:\n",
        "            raise Exception('Please run accumulate() first')\n",
        "        iouType = self.params.iouType\n",
        "        if iouType == 'segm' or iouType == 'bbox':\n",
        "            summarize = _summarizeDets\n",
        "        elif iouType == 'keypoints':\n",
        "            summarize = _summarizeKps\n",
        "        self.stats = summarize()\n",
        "\n",
        "    def __str__(self):\n",
        "        self.summarize()\n",
        "\n",
        "class Params:\n",
        "    '''\n",
        "    Params for coco evaluation api\n",
        "    '''\n",
        "    def setDetParams(self):\n",
        "        self.imgIds = []\n",
        "        self.catIds = []\n",
        "        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n",
        "        self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
        "        self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)\n",
        "        self.maxDets = [1, 10, 100]\n",
        "        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n",
        "        self.areaRngLbl = ['all', 'small', 'medium', 'large']\n",
        "        self.useCats = 1\n",
        "\n",
        "    def setKpParams(self):\n",
        "        self.imgIds = []\n",
        "        self.catIds = []\n",
        "        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n",
        "        self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
        "        self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)\n",
        "        self.maxDets = [20]\n",
        "        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n",
        "        self.areaRngLbl = ['all', 'medium', 'large']\n",
        "        self.useCats = 1\n",
        "        self.kpt_oks_sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n",
        "\n",
        "    def __init__(self, iouType='segm'):\n",
        "        if iouType == 'segm' or iouType == 'bbox':\n",
        "            self.setDetParams()\n",
        "        elif iouType == 'keypoints':\n",
        "            self.setKpParams()\n",
        "        else:\n",
        "            raise Exception('iouType not supported')\n",
        "        self.iouType = iouType\n",
        "        # useSegm is deprecated\n",
        "        self.useSegm = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daFT_1DOH-Rd"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates (Wu, 2019)\n",
        "###Adapted from cocoeval source code (Lin et al., 2014)\n",
        "import contextlib\n",
        "import copy\n",
        "import io\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import pycocotools.mask as mask_util\n",
        "import torch\n",
        "from pycocotools.coco import COCO\n",
        "from tabulate import tabulate\n",
        "\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.config import CfgNode\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.data.datasets.coco import convert_to_coco_json\n",
        "from detectron2.structures import Boxes, BoxMode, pairwise_iou\n",
        "from detectron2.utils.file_io import PathManager\n",
        "from detectron2.utils.logger import create_small_table\n",
        "import seaborn as sns\n",
        "\n",
        "# from cocoeval import COCOeval1\n",
        "# from evaluator import DatasetEvaluator1\n",
        "\n",
        "try:\n",
        "    from detectron2.evaluation.fast_eval_api import COCOeval_opt\n",
        "except ImportError:\n",
        "    COCOeval_opt = COCOeval1\n",
        "\n",
        "\n",
        "class COCOEvaluator1(DatasetEvaluator1):\n",
        "    \"\"\"\n",
        "    Evaluate AR for object proposals, AP for instance detection/segmentation, AP\n",
        "    for keypoint detection outputs using COCO's metrics.\n",
        "    See http://cocodataset.org/#detection-eval and\n",
        "    http://cocodataset.org/#keypoints-eval to understand its metrics.\n",
        "    The metrics range from 0 to 100 (instead of 0 to 1), where a -1 or NaN means\n",
        "    the metric cannot be computed (e.g. due to no predictions made).\n",
        "\n",
        "    In addition to COCO, this evaluator is able to support any bounding box detection,\n",
        "    instance segmentation, or keypoint detection dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name,\n",
        "        tasks=None,\n",
        "        distributed=True,\n",
        "        output_dir=None,\n",
        "        *,\n",
        "        max_dets_per_image=None,\n",
        "        use_fast_impl=True,\n",
        "        kpt_oks_sigmas=(),\n",
        "        allow_cached_coco=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_name (str): name of the dataset to be evaluated.\n",
        "                It must have either the following corresponding metadata:\n",
        "\n",
        "                    \"json_file\": the path to the COCO format annotation\n",
        "\n",
        "                Or it must be in detectron2's standard dataset format\n",
        "                so it can be converted to COCO format automatically.\n",
        "            tasks (tuple[str]): tasks that can be evaluated under the given\n",
        "                configuration. A task is one of \"bbox\", \"segm\", \"keypoints\".\n",
        "                By default, will infer this automatically from predictions.\n",
        "            distributed (True): if True, will collect results from all ranks and run evaluation\n",
        "                in the main process.\n",
        "                Otherwise, will only evaluate the results in the current process.\n",
        "            output_dir (str): optional, an output directory to dump all\n",
        "                results predicted on the dataset. The dump contains two files:\n",
        "\n",
        "                1. \"instances_predictions.pth\" a file that can be loaded with `torch.load` and\n",
        "                   contains all the results in the format they are produced by the model.\n",
        "                2. \"coco_instances_results.json\" a json file in COCO's result format.\n",
        "            max_dets_per_image (int): limit on the maximum number of detections per image.\n",
        "                By default in COCO, this limit is to 100, but this can be customized\n",
        "                to be greater, as is needed in evaluation metrics AP fixed and AP pool\n",
        "                (see https://arxiv.org/pdf/2102.01066.pdf)\n",
        "                This doesn't affect keypoint evaluation.\n",
        "            use_fast_impl (bool): use a fast but **unofficial** implementation to compute AP.\n",
        "                Although the results should be very close to the official implementation in COCO\n",
        "                API, it is still recommended to compute results with the official API for use in\n",
        "                papers. The faster implementation also uses more RAM.\n",
        "            kpt_oks_sigmas (list[float]): The sigmas used to calculate keypoint OKS.\n",
        "                See http://cocodataset.org/#keypoints-eval\n",
        "                When empty, it will use the defaults in COCO.\n",
        "                Otherwise it should be the same length as ROI_KEYPOINT_HEAD.NUM_KEYPOINTS.\n",
        "            allow_cached_coco (bool): Whether to use cached coco json from previous validation\n",
        "                runs. You should set this to False if you need to use different validation data.\n",
        "                Defaults to True.\n",
        "        \"\"\"\n",
        "        self._logger = logging.getLogger(__name__)\n",
        "        self._distributed = distributed\n",
        "        self._output_dir = output_dir\n",
        "\n",
        "        if use_fast_impl and (COCOeval_opt is COCOeval1):\n",
        "            self._logger.info(\"Fast COCO eval is not built. Falling back to official COCO eval.\")\n",
        "            use_fast_impl = False\n",
        "        self._use_fast_impl = use_fast_impl\n",
        "\n",
        "        # COCOeval requires the limit on the number of detections per image (maxDets) to be a list\n",
        "        # with at least 3 elements. The default maxDets in COCOeval is [1, 10, 100], in which the\n",
        "        # 3rd element (100) is used as the limit on the number of detections per image when\n",
        "        # evaluating AP. COCOEvaluator expects an integer for max_dets_per_image, so for COCOeval,\n",
        "        # we reformat max_dets_per_image into [1, 10, max_dets_per_image], based on the defaults.\n",
        "        if max_dets_per_image is None:\n",
        "            max_dets_per_image = [1, 10, 100]\n",
        "        else:\n",
        "            max_dets_per_image = [1, 10, max_dets_per_image]\n",
        "        self._max_dets_per_image = max_dets_per_image\n",
        "\n",
        "        if tasks is not None and isinstance(tasks, CfgNode):\n",
        "            kpt_oks_sigmas = (\n",
        "                tasks.TEST.KEYPOINT_OKS_SIGMAS if not kpt_oks_sigmas else kpt_oks_sigmas\n",
        "            )\n",
        "            self._logger.warn(\n",
        "                \"COCO Evaluator instantiated using config, this is deprecated behavior.\"\n",
        "                \" Please pass in explicit arguments instead.\"\n",
        "            )\n",
        "            self._tasks = None  # Infering it from predictions should be better\n",
        "        else:\n",
        "            self._tasks = tasks\n",
        "\n",
        "        self._cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "        self._metadata = MetadataCatalog.get(dataset_name)\n",
        "        if not hasattr(self._metadata, \"json_file\"):\n",
        "            if output_dir is None:\n",
        "                raise ValueError(\n",
        "                    \"output_dir must be provided to COCOEvaluator \"\n",
        "                    \"for datasets not in COCO format.\"\n",
        "                )\n",
        "            self._logger.info(f\"Trying to convert '{dataset_name}' to COCO format ...\")\n",
        "\n",
        "            cache_path = os.path.join(output_dir, f\"{dataset_name}_coco_format.json\")\n",
        "            self._metadata.json_file = cache_path\n",
        "            convert_to_coco_json(dataset_name, cache_path, allow_cached=allow_cached_coco)\n",
        "\n",
        "        json_file = PathManager.get_local_path(self._metadata.json_file)\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            self._coco_api = COCO(json_file)\n",
        "\n",
        "        # Test set json files do not contain annotations (evaluation must be\n",
        "        # performed using the COCO evaluation server).\n",
        "        self._do_evaluation = \"annotations\" in self._coco_api.dataset\n",
        "        if self._do_evaluation:\n",
        "            self._kpt_oks_sigmas = kpt_oks_sigmas\n",
        "\n",
        "    def reset(self):\n",
        "        self._predictions = []\n",
        "\n",
        "    def process(self, inputs, outputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).\n",
        "                It is a list of dict. Each dict corresponds to an image and\n",
        "                contains keys like \"height\", \"width\", \"file_name\", \"image_id\".\n",
        "            outputs: the outputs of a COCO model. It is a list of dicts with key\n",
        "                \"instances\" that contains :class:`Instances`.\n",
        "        \"\"\"\n",
        "        for input, output in zip(inputs, outputs):\n",
        "            prediction = {\"image_id\": input[\"image_id\"]}\n",
        "\n",
        "            if \"instances\" in output:\n",
        "                instances = output[\"instances\"].to(self._cpu_device)\n",
        "                prediction[\"instances\"] = instances_to_coco_json(instances, input[\"image_id\"])\n",
        "            if \"proposals\" in output:\n",
        "                prediction[\"proposals\"] = output[\"proposals\"].to(self._cpu_device)\n",
        "            if len(prediction) > 1:\n",
        "                self._predictions.append(prediction)\n",
        "\n",
        "    def evaluate(self, img_ids=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_ids: a list of image IDs to evaluate on. Default to None for the whole dataset\n",
        "        \"\"\"\n",
        "        if self._distributed:\n",
        "            comm.synchronize()\n",
        "            predictions = comm.gather(self._predictions, dst=0)\n",
        "            predictions = list(itertools.chain(*predictions))\n",
        "\n",
        "            if not comm.is_main_process():\n",
        "                return {}\n",
        "        else:\n",
        "            predictions = self._predictions\n",
        "\n",
        "        if len(predictions) == 0:\n",
        "            self._logger.warning(\"[COCOEvaluator] Did not receive valid predictions.\")\n",
        "            return {}\n",
        "\n",
        "        if self._output_dir:\n",
        "            PathManager.mkdirs(self._output_dir)\n",
        "            file_path = os.path.join(self._output_dir, \"instances_predictions.pth\")\n",
        "            with PathManager.open(file_path, \"wb\") as f:\n",
        "                torch.save(predictions, f)\n",
        "\n",
        "        self._results = OrderedDict()\n",
        "        if \"proposals\" in predictions[0]:\n",
        "            self._eval_box_proposals(predictions)\n",
        "        if \"instances\" in predictions[0]:\n",
        "            self._eval_predictions(predictions, img_ids=img_ids)\n",
        "        # Copy so the caller can do whatever with results\n",
        "        return copy.deepcopy(self._results)\n",
        "\n",
        "    def _tasks_from_predictions(self, predictions):\n",
        "        \"\"\"\n",
        "        Get COCO API \"tasks\" (i.e. iou_type) from COCO-format predictions.\n",
        "        \"\"\"\n",
        "        tasks = {\"bbox\"}\n",
        "        for pred in predictions:\n",
        "            if \"segmentation\" in pred:\n",
        "                tasks.add(\"segm\")\n",
        "            if \"keypoints\" in pred:\n",
        "                tasks.add(\"keypoints\")\n",
        "        return sorted(tasks)\n",
        "\n",
        "    def _eval_predictions(self, predictions, img_ids=None):\n",
        "        \"\"\"\n",
        "        Evaluate predictions. Fill self._results with the metrics of the tasks.\n",
        "        \"\"\"\n",
        "        self._logger.info(\"Preparing results for COCO format ...\")\n",
        "        coco_results = list(itertools.chain(*[x[\"instances\"] for x in predictions]))\n",
        "        tasks = self._tasks or self._tasks_from_predictions(coco_results)\n",
        "\n",
        "        # unmap the category ids for COCO\n",
        "        if hasattr(self._metadata, \"thing_dataset_id_to_contiguous_id\"):\n",
        "            dataset_id_to_contiguous_id = self._metadata.thing_dataset_id_to_contiguous_id\n",
        "            all_contiguous_ids = list(dataset_id_to_contiguous_id.values())\n",
        "            num_classes = len(all_contiguous_ids)\n",
        "            assert min(all_contiguous_ids) == 0 and max(all_contiguous_ids) == num_classes - 1\n",
        "\n",
        "            reverse_id_mapping = {v: k for k, v in dataset_id_to_contiguous_id.items()}\n",
        "            for result in coco_results:\n",
        "                category_id = result[\"category_id\"]\n",
        "                assert category_id < num_classes, (\n",
        "                    f\"A prediction has class={category_id}, \"\n",
        "                    f\"but the dataset only has {num_classes} classes and \"\n",
        "                    f\"predicted class id should be in [0, {num_classes - 1}].\"\n",
        "                )\n",
        "                result[\"category_id\"] = reverse_id_mapping[category_id]\n",
        "\n",
        "        if self._output_dir:\n",
        "            file_path = os.path.join(self._output_dir, \"coco_instances_results.json\")\n",
        "            self._logger.info(\"Saving results to {}\".format(file_path))\n",
        "            with PathManager.open(file_path, \"w\") as f:\n",
        "                f.write(json.dumps(coco_results))\n",
        "                f.flush()\n",
        "\n",
        "        if not self._do_evaluation:\n",
        "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
        "            return\n",
        "\n",
        "        self._logger.info(\n",
        "            \"Evaluating predictions with {} COCO API...\".format(\n",
        "                \"unofficial\" if self._use_fast_impl else \"official\"\n",
        "            )\n",
        "        )\n",
        "        for task in sorted(tasks):\n",
        "            assert task in {\"bbox\", \"segm\", \"keypoints\"}, f\"Got unknown task: {task}!\"\n",
        "            coco_eval = (\n",
        "                _evaluate_predictions_on_coco(\n",
        "                    self._coco_api,\n",
        "                    coco_results,\n",
        "                    task,\n",
        "                    kpt_oks_sigmas=self._kpt_oks_sigmas,\n",
        "                    cocoeval_fn=COCOeval_opt if self._use_fast_impl else COCOeval1,\n",
        "                    img_ids=img_ids,\n",
        "                    max_dets_per_image=self._max_dets_per_image,\n",
        "                    class_names=self._metadata.get(\"thing_classes\")\n",
        "                )\n",
        "                if len(coco_results) > 0\n",
        "                else None  # cocoapi does not handle empty results very well\n",
        "            )\n",
        "\n",
        "            res = self._derive_coco_results(\n",
        "                coco_eval, task, class_names=self._metadata.get(\"thing_classes\")\n",
        "            )\n",
        "            self._results[task] = res\n",
        "\n",
        "    def _eval_box_proposals(self, predictions):\n",
        "        \"\"\"\n",
        "        Evaluate the box proposals in predictions.\n",
        "        Fill self._results with the metrics for \"box_proposals\" task.\n",
        "        \"\"\"\n",
        "        if self._output_dir:\n",
        "            # Saving generated box proposals to file.\n",
        "            # Predicted box_proposals are in XYXY_ABS mode.\n",
        "            bbox_mode = BoxMode.XYXY_ABS.value\n",
        "            ids, boxes, objectness_logits = [], [], []\n",
        "            for prediction in predictions:\n",
        "                ids.append(prediction[\"image_id\"])\n",
        "                boxes.append(prediction[\"proposals\"].proposal_boxes.tensor.numpy())\n",
        "                objectness_logits.append(prediction[\"proposals\"].objectness_logits.numpy())\n",
        "\n",
        "            proposal_data = {\n",
        "                \"boxes\": boxes,\n",
        "                \"objectness_logits\": objectness_logits,\n",
        "                \"ids\": ids,\n",
        "                \"bbox_mode\": bbox_mode,\n",
        "            }\n",
        "            with PathManager.open(os.path.join(self._output_dir, \"box_proposals.pkl\"), \"wb\") as f:\n",
        "                pickle.dump(proposal_data, f)\n",
        "\n",
        "        if not self._do_evaluation:\n",
        "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
        "            return\n",
        "\n",
        "        self._logger.info(\"Evaluating bbox proposals ...\")\n",
        "        res = {}\n",
        "        areas = {\"all\": \"\", \"small\": \"s\", \"medium\": \"m\", \"large\": \"l\"}\n",
        "        for limit in [100, 1000]:\n",
        "            for area, suffix in areas.items():\n",
        "                stats = _evaluate_box_proposals(predictions, self._coco_api, area=area, limit=limit)\n",
        "                key = \"AR{}@{:d}\".format(suffix, limit)\n",
        "                res[key] = float(stats[\"ar\"].item() * 100)\n",
        "        self._logger.info(\"Proposal metrics: \\n\" + create_small_table(res))\n",
        "        self._results[\"box_proposals\"] = res\n",
        "\n",
        "    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):\n",
        "        \"\"\"\n",
        "        Derive the desired score numbers from summarized COCOeval.\n",
        "\n",
        "        Args:\n",
        "            coco_eval (None or COCOEval): None represents no predictions from model.\n",
        "            iou_type (str):\n",
        "            class_names (None or list[str]): if provided, will use it to predict\n",
        "                per-category AP.\n",
        "\n",
        "        Returns:\n",
        "            a dict of {metric name: score}\n",
        "        \"\"\"\n",
        "\n",
        "        metrics = {\n",
        "            \"bbox\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
        "            \"segm\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
        "            \"keypoints\": [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"],\n",
        "        }[iou_type]\n",
        "\n",
        "        if coco_eval is None:\n",
        "            self._logger.warn(\"No predictions from the model!\")\n",
        "            return {metric: float(\"nan\") for metric in metrics}\n",
        "\n",
        "        # the standard metrics\n",
        "        results = {\n",
        "            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else \"nan\")\n",
        "            for idx, metric in enumerate(metrics)\n",
        "        }\n",
        "        self._logger.info(\n",
        "            \"Evaluation results for {}: \\n\".format(iou_type) + create_small_table(results)\n",
        "        )\n",
        "        if not np.isfinite(sum(results.values())):\n",
        "            self._logger.info(\"Some metrics cannot be computed and is shown as NaN.\")\n",
        "\n",
        "        if class_names is None or len(class_names) <= 1:\n",
        "            return results\n",
        "        # Compute per-category AP\n",
        "        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa\n",
        "        precisions = coco_eval.eval[\"precision\"] ##\n",
        "        # precision has dims (iou, recall, cls, area range, max dets)\n",
        "        assert len(class_names) == precisions.shape[2]\n",
        "\n",
        "        results_per_category = []\n",
        "        for idx, name in enumerate(class_names):\n",
        "            # area range index 0: all area ranges\n",
        "            # max dets index -1: typically 100 per image\n",
        "            precision = precisions[:, :, idx, 0, -1]\n",
        "            precision = precision[precision > -1]\n",
        "            ap = np.mean(precision) if precision.size else float(\"nan\")\n",
        "            results_per_category.append((\"{}\".format(name), float(ap * 100))) ##\n",
        "\n",
        "        # tabulate it\n",
        "        N_COLS = min(6, len(results_per_category) * 2) ##\n",
        "        results_flatten = list(itertools.chain(*results_per_category))\n",
        "        results_2d = itertools.zip_longest(*[results_flatten[i::N_COLS] for i in range(N_COLS)])\n",
        "        table = tabulate(\n",
        "            results_2d,\n",
        "            tablefmt=\"pipe\",\n",
        "            floatfmt=\".3f\",\n",
        "            headers=[\"category\", \"AP\"] * (N_COLS // 2), ##\n",
        "            numalign=\"left\",\n",
        "        )\n",
        "        self._logger.info(\"Per-category {} AP: \\n\".format(iou_type) + table)\n",
        "\n",
        "        results.update({\"AP-\" + name: ap for name, ap in results_per_category}) ##\n",
        "        return results\n",
        "\n",
        "\n",
        "def instances_to_coco_json(instances, img_id):\n",
        "    \"\"\"\n",
        "    Dump an \"Instances\" object to a COCO-format json that's used for evaluation.\n",
        "\n",
        "    Args:\n",
        "        instances (Instances):\n",
        "        img_id (int): the image id\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: list of json annotations in COCO format.\n",
        "    \"\"\"\n",
        "    num_instance = len(instances)\n",
        "    if num_instance == 0:\n",
        "        return []\n",
        "\n",
        "    boxes = instances.pred_boxes.tensor.numpy()\n",
        "    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n",
        "    boxes = boxes.tolist()\n",
        "    scores = instances.scores.tolist()\n",
        "    classes = instances.pred_classes.tolist()\n",
        "\n",
        "    has_mask = instances.has(\"pred_masks\")\n",
        "    if has_mask:\n",
        "        # use RLE to encode the masks, because they are too large and takes memory\n",
        "        # since this evaluator stores outputs of the entire dataset\n",
        "        rles = [\n",
        "            mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n",
        "            for mask in instances.pred_masks\n",
        "        ]\n",
        "        for rle in rles:\n",
        "            # \"counts\" is an array encoded by mask_util as a byte-stream. Python3's\n",
        "            # json writer which always produces strings cannot serialize a bytestream\n",
        "            # unless you decode it. Thankfully, utf-8 works out (which is also what\n",
        "            # the pycocotools/_mask.pyx does).\n",
        "            rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "    has_keypoints = instances.has(\"pred_keypoints\")\n",
        "    if has_keypoints:\n",
        "        keypoints = instances.pred_keypoints\n",
        "\n",
        "    results = []\n",
        "    for k in range(num_instance):\n",
        "        result = {\n",
        "            \"image_id\": img_id,\n",
        "            \"category_id\": classes[k],\n",
        "            \"bbox\": boxes[k],\n",
        "            \"score\": scores[k],\n",
        "        }\n",
        "        if has_mask:\n",
        "            result[\"segmentation\"] = rles[k]\n",
        "        if has_keypoints:\n",
        "            # In COCO annotations,\n",
        "            # keypoints coordinates are pixel indices.\n",
        "            # However our predictions are floating point coordinates.\n",
        "            # Therefore we subtract 0.5 to be consistent with the annotation format.\n",
        "            # This is the inverse of data loading logic in `datasets/coco.py`.\n",
        "            keypoints[k][:, :2] -= 0.5\n",
        "            result[\"keypoints\"] = keypoints[k].flatten().tolist()\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "\n",
        "# inspired from Detectron:\n",
        "# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa\n",
        "def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area=\"all\", limit=None):\n",
        "    \"\"\"\n",
        "    Evaluate detection proposal recall metrics. This function is a much\n",
        "    faster alternative to the official COCO API recall evaluation code. However,\n",
        "    it produces slightly different results.\n",
        "    \"\"\"\n",
        "    # Record max overlap value for each gt box\n",
        "    # Return vector of overlap values\n",
        "    areas = {\n",
        "        \"all\": 0,\n",
        "        \"small\": 1,\n",
        "        \"medium\": 2,\n",
        "        \"large\": 3,\n",
        "        \"96-128\": 4,\n",
        "        \"128-256\": 5,\n",
        "        \"256-512\": 6,\n",
        "        \"512-inf\": 7,\n",
        "    }\n",
        "    area_ranges = [\n",
        "        [0**2, 1e5**2],  # all\n",
        "        [0**2, 32**2],  # small\n",
        "        [32**2, 96**2],  # medium\n",
        "        [96**2, 1e5**2],  # large\n",
        "        [96**2, 128**2],  # 96-128\n",
        "        [128**2, 256**2],  # 128-256\n",
        "        [256**2, 512**2],  # 256-512\n",
        "        [512**2, 1e5**2],\n",
        "    ]  # 512-inf\n",
        "    assert area in areas, \"Unknown area range: {}\".format(area)\n",
        "    area_range = area_ranges[areas[area]]\n",
        "    gt_overlaps = []\n",
        "    num_pos = 0\n",
        "\n",
        "    for prediction_dict in dataset_predictions:\n",
        "        predictions = prediction_dict[\"proposals\"]\n",
        "\n",
        "        # sort predictions in descending order\n",
        "        # TODO maybe remove this and make it explicit in the documentation\n",
        "        inds = predictions.objectness_logits.sort(descending=True)[1]\n",
        "        predictions = predictions[inds]\n",
        "\n",
        "        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[\"image_id\"])\n",
        "        anno = coco_api.loadAnns(ann_ids)\n",
        "        gt_boxes = [\n",
        "            BoxMode.convert(obj[\"bbox\"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
        "            for obj in anno\n",
        "            if obj[\"iscrowd\"] == 0\n",
        "        ]\n",
        "        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes\n",
        "        gt_boxes = Boxes(gt_boxes)\n",
        "        gt_areas = torch.as_tensor([obj[\"area\"] for obj in anno if obj[\"iscrowd\"] == 0])\n",
        "\n",
        "        if len(gt_boxes) == 0 or len(predictions) == 0:\n",
        "            continue\n",
        "\n",
        "        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])\n",
        "        gt_boxes = gt_boxes[valid_gt_inds]\n",
        "\n",
        "        num_pos += len(gt_boxes)\n",
        "\n",
        "        if len(gt_boxes) == 0:\n",
        "            continue\n",
        "\n",
        "        if limit is not None and len(predictions) > limit:\n",
        "            predictions = predictions[:limit]\n",
        "\n",
        "        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)\n",
        "\n",
        "        _gt_overlaps = torch.zeros(len(gt_boxes))\n",
        "        for j in range(min(len(predictions), len(gt_boxes))):\n",
        "            # find which proposal box maximally covers each gt box\n",
        "            # and get the iou amount of coverage for each gt box\n",
        "            max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n",
        "\n",
        "            # find which gt box is 'best' covered (i.e. 'best' = most iou)\n",
        "            gt_ovr, gt_ind = max_overlaps.max(dim=0)\n",
        "            assert gt_ovr >= 0\n",
        "            # find the proposal box that covers the best covered gt box\n",
        "            box_ind = argmax_overlaps[gt_ind]\n",
        "            # record the iou coverage of this gt box\n",
        "            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n",
        "            assert _gt_overlaps[j] == gt_ovr\n",
        "            # mark the proposal box and the gt box as used\n",
        "            overlaps[box_ind, :] = -1\n",
        "            overlaps[:, gt_ind] = -1\n",
        "\n",
        "        # append recorded iou coverage level\n",
        "        gt_overlaps.append(_gt_overlaps)\n",
        "    gt_overlaps = (\n",
        "        torch.cat(gt_overlaps, dim=0) if len(gt_overlaps) else torch.zeros(0, dtype=torch.float32)\n",
        "    )\n",
        "    gt_overlaps, _ = torch.sort(gt_overlaps)\n",
        "\n",
        "    if thresholds is None:\n",
        "        step = 0.05\n",
        "        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)\n",
        "    recalls = torch.zeros_like(thresholds)\n",
        "    # compute recall for each iou threshold\n",
        "    for i, t in enumerate(thresholds):\n",
        "        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)\n",
        "    # ar = 2 * np.trapz(recalls, thresholds)\n",
        "    ar = recalls.mean()\n",
        "    return {\n",
        "        \"ar\": ar,\n",
        "        \"recalls\": recalls,\n",
        "        \"thresholds\": thresholds,\n",
        "        \"gt_overlaps\": gt_overlaps,\n",
        "        \"num_pos\": num_pos,\n",
        "    }\n",
        "\n",
        "\n",
        "def _evaluate_predictions_on_coco(\n",
        "    coco_gt,\n",
        "    coco_results,\n",
        "    iou_type,\n",
        "    kpt_oks_sigmas=None,\n",
        "    cocoeval_fn=COCOeval_opt,\n",
        "    img_ids=None,\n",
        "    max_dets_per_image=None,\n",
        "    class_names = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate the coco results using COCOEval API.\n",
        "    \"\"\"\n",
        "    assert len(coco_results) > 0\n",
        "\n",
        "    if iou_type == \"segm\":\n",
        "        coco_results = copy.deepcopy(coco_results)\n",
        "        # When evaluating mask AP, if the results contain bbox, cocoapi will\n",
        "        # use the box area as the area of the instance, instead of the mask area.\n",
        "        # This leads to a different definition of small/medium/large.\n",
        "        # We remove the bbox field to let mask AP use mask area.\n",
        "        for c in coco_results:\n",
        "            c.pop(\"bbox\", None)\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(coco_results)\n",
        "    coco_eval = cocoeval_fn(coco_gt, coco_dt, iou_type, class_names=class_names)\n",
        "    # For COCO, the default max_dets_per_image is [1, 10, 100].\n",
        "    if max_dets_per_image is None:\n",
        "        max_dets_per_image = [1, 10, 100]  # Default from COCOEval\n",
        "    else:\n",
        "        assert (\n",
        "            len(max_dets_per_image) >= 3\n",
        "        ), \"COCOeval requires maxDets (and max_dets_per_image) to have length at least 3\"\n",
        "        # In the case that user supplies a custom input for max_dets_per_image,\n",
        "        # apply COCOevalMaxDets to evaluate AP with the custom input.\n",
        "        if max_dets_per_image[2] != 100:\n",
        "            coco_eval = COCOevalMaxDets(coco_gt, coco_dt, iou_type)\n",
        "    if iou_type != \"keypoints\":\n",
        "        coco_eval.params.maxDets = max_dets_per_image\n",
        "\n",
        "    if img_ids is not None:\n",
        "        coco_eval.params.imgIds = img_ids\n",
        "\n",
        "    if iou_type == \"keypoints\":\n",
        "        # Use the COCO default keypoint OKS sigmas unless overrides are specified\n",
        "        if kpt_oks_sigmas:\n",
        "            assert hasattr(coco_eval.params, \"kpt_oks_sigmas\"), \"pycocotools is too old!\"\n",
        "            coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)\n",
        "        # COCOAPI requires every detection and every gt to have keypoints, so\n",
        "        # we just take the first entry from both\n",
        "        num_keypoints_dt = len(coco_results[0][\"keypoints\"]) // 3\n",
        "        num_keypoints_gt = len(next(iter(coco_gt.anns.values()))[\"keypoints\"]) // 3\n",
        "        num_keypoints_oks = len(coco_eval.params.kpt_oks_sigmas)\n",
        "        assert num_keypoints_oks == num_keypoints_dt == num_keypoints_gt, (\n",
        "            f\"[COCOEvaluator] Prediction contain {num_keypoints_dt} keypoints. \"\n",
        "            f\"Ground truth contains {num_keypoints_gt} keypoints. \"\n",
        "            f\"The length of cfg.TEST.KEYPOINT_OKS_SIGMAS is {num_keypoints_oks}. \"\n",
        "            \"They have to agree with each other. For meaning of OKS, please refer to \"\n",
        "            \"http://cocodataset.org/#keypoints-eval.\"\n",
        "        )\n",
        "\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "    return coco_eval\n",
        "\n",
        "class COCOevalMaxDets(COCOeval1):\n",
        "    \"\"\"\n",
        "    Modified version of COCOeval for evaluating AP with a custom\n",
        "    maxDets (by default for COCO, maxDets is 100)\n",
        "    \"\"\"\n",
        "\n",
        "    def summarize(self):\n",
        "        \"\"\"\n",
        "        Compute and display summary metrics for evaluation results given\n",
        "        a custom value for  max_dets_per_image\n",
        "        \"\"\"\n",
        "\n",
        "        def _summarize(ap=1, iouThr=None, areaRng=\"all\", maxDets=100):\n",
        "            p = self.params\n",
        "            iStr = \" {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\"\n",
        "            titleStr = \"Average Precision\" if ap == 1 else \"Average Recall\"\n",
        "            typeStr = \"(AP)\" if ap == 1 else \"(AR)\"\n",
        "            iouStr = (\n",
        "                \"{:0.2f}:{:0.2f}\".format(p.iouThrs[0], p.iouThrs[-1])\n",
        "                if iouThr is None\n",
        "                else \"{:0.2f}\".format(iouThr)\n",
        "            )\n",
        "\n",
        "            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
        "            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
        "            if ap == 1:\n",
        "                # dimension of precision: [TxRxKxAxM]\n",
        "                s = self.eval[\"precision\"]\n",
        "                # IoU\n",
        "                if iouThr is not None:\n",
        "                    t = np.where(iouThr == p.iouThrs)[0]\n",
        "                    s = s[t]\n",
        "                s = s[:, :, :, aind, mind]\n",
        "            else:\n",
        "                # dimension of recall: [TxKxAxM]\n",
        "                s = self.eval[\"recall\"]\n",
        "                if iouThr is not None:\n",
        "                    t = np.where(iouThr == p.iouThrs)[0]\n",
        "                    s = s[t]\n",
        "                s = s[:, :, aind, mind]\n",
        "            if len(s[s > -1]) == 0:\n",
        "                mean_s = -1\n",
        "            else:\n",
        "                mean_s = np.mean(s[s > -1])\n",
        "            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
        "            return mean_s\n",
        "\n",
        "        def _summarizeDets():\n",
        "            stats = np.zeros((12,))\n",
        "            # Evaluate AP using the custom limit on maximum detections per image\n",
        "            stats[0] = _summarize(1, maxDets=self.params.maxDets[2])\n",
        "            stats[1] = _summarize(1, iouThr=0.5, maxDets=self.params.maxDets[2])\n",
        "            stats[2] = _summarize(1, iouThr=0.75, maxDets=self.params.maxDets[2])\n",
        "            stats[3] = _summarize(1, areaRng=\"small\", maxDets=self.params.maxDets[2])\n",
        "            stats[4] = _summarize(1, areaRng=\"medium\", maxDets=self.params.maxDets[2])\n",
        "            stats[5] = _summarize(1, areaRng=\"large\", maxDets=self.params.maxDets[2])\n",
        "            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n",
        "            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n",
        "            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n",
        "            stats[9] = _summarize(0, areaRng=\"small\", maxDets=self.params.maxDets[2])\n",
        "            stats[10] = _summarize(0, areaRng=\"medium\", maxDets=self.params.maxDets[2])\n",
        "            stats[11] = _summarize(0, areaRng=\"large\", maxDets=self.params.maxDets[2])\n",
        "            return stats\n",
        "\n",
        "        def _summarizeKps():\n",
        "            stats = np.zeros((10,))\n",
        "            stats[0] = _summarize(1, maxDets=20)\n",
        "            stats[1] = _summarize(1, maxDets=20, iouThr=0.5)\n",
        "            stats[2] = _summarize(1, maxDets=20, iouThr=0.75)\n",
        "            stats[3] = _summarize(1, maxDets=20, areaRng=\"medium\")\n",
        "            stats[4] = _summarize(1, maxDets=20, areaRng=\"large\")\n",
        "            stats[5] = _summarize(0, maxDets=20)\n",
        "            stats[6] = _summarize(0, maxDets=20, iouThr=0.5)\n",
        "            stats[7] = _summarize(0, maxDets=20, iouThr=0.75)\n",
        "            stats[8] = _summarize(0, maxDets=20, areaRng=\"medium\")\n",
        "            stats[9] = _summarize(0, maxDets=20, areaRng=\"large\")\n",
        "            return stats\n",
        "\n",
        "        if not self.eval:\n",
        "            raise Exception(\"Please run accumulate() first\")\n",
        "        iouType = self.params.iouType\n",
        "        if iouType == \"segm\" or iouType == \"bbox\":\n",
        "            summarize = _summarizeDets\n",
        "        elif iouType == \"keypoints\":\n",
        "            summarize = _summarizeKps\n",
        "        self.stats = summarize()\n",
        "\n",
        "    def __str__(self):\n",
        "        self.summarize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXJgbuUx82JI"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uJnC2-bhCqQ"
      },
      "outputs": [],
      "source": [
        "#Evaluate Model, Output presence based metrics and confusion matrix\n",
        "evaluator = COCOEvaluator1(\"my_dataset_test\", cfg, False, output_dir=\"./output\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
        "inference_on_dataset(predictor.model, val_loader, evaluator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9EKto5OM1CU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}